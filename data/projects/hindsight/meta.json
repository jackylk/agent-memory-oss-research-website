{
  "name": "hindsight",
  "repository_url": "https://github.com/vectorize-io/hindsight",
  "stars": 1400,
  "primary_language": "Python",
  "description": "专为 AI Agent 设计的仿生记忆系统,通过模拟人类记忆机制实现智能体的长期学习能力,而非简单的对话历史存储",
  "last_updated": "2025-10",
  "paper": {
    "exists": true,
    "title": "Hindsight: Learning-focused Agent Memory",
    "venue": "Research Paper",
    "year": 2025,
    "url": "https://arxiv.org/abs/2512.12818"
  },
  "benchmarks": {
    "longmemeval": {
      "score": 0,
      "details": "State-of-the-art performance on LongMemEval, independently reproduced by Virginia Tech and Washington Post"
    }
  },
  "tech_stack": {
    "storage": [
      "PostgreSQL",
      "Graph Database",
      "Vector Store"
    ],
    "frameworks": [
      "Python",
      "HTTP API",
      "Docker"
    ],
    "languages": [
      "Python"
    ],
    "embedding_models": [
      "Semantic + BM25 + Graph + Temporal"
    ]
  },
  "cloud_needs": {
    "mandatory_cloud_services": [
      {
        "type": "PostgreSQL Database",
        "providers": [
          "Huawei RDS PostgreSQL",
          "AWS RDS",
          "Google Cloud SQL",
          "Self-hosted"
        ],
        "purpose": "Primary database with pgvector extension for vector storage",
        "cost": "$300-1800/month",
        "necessity": "Mandatory",
        "extensions_required": [
          "pgvector",
          "pg_trgm"
        ]
      },
      {
        "type": "LLM API",
        "providers": [
          "OpenAI",
          "Anthropic",
          "Google Gemini",
          "Groq",
          "Ollama (local)"
        ],
        "purpose": "Fact extraction, reflection, consolidation",
        "cost": "$40-500/month",
        "necessity": "Mandatory"
      }
    ],
    "optional_cloud_services": [
      {
        "type": "Object Storage",
        "providers": [
          "Huawei OBS",
          "AWS S3",
          "Google Cloud Storage"
        ],
        "purpose": "Database backup and log archival",
        "cost": "$10-200/month",
        "necessity": "Optional"
      },
      {
        "type": "Embedding API",
        "providers": [
          "OpenAI Embeddings",
          "Cohere",
          "TEI (self-hosted)",
          "Local models (default)"
        ],
        "purpose": "Alternative to local embedding models",
        "cost": "$0-200/month",
        "necessity": "Optional"
      }
    ],
    "storage": {
      "types": [
        "PostgreSQL",
        "Graph DB",
        "Vector Store"
      ],
      "requirements": [
        "Multi-strategy retrieval",
        "Biomimetic structures"
      ],
      "graph_database": {
        "required": false,
        "type": "PostgreSQL tables (entities, entity_links, memory_links)",
        "note": "Graph functionality implemented in SQL, no standalone graph DB needed"
      },
      "object_storage": {
        "required": false,
        "use_case": "Backup only, not core functionality"
      }
    },
    "compute": {
      "embedding": true,
      "gpu_needed": false,
      "gpu_optional": true,
      "estimated_requirements": "8-16 vCPUs for learning operations",
      "gpu_acceleration_notes": "Optional for local models (bge-small-en-v1.5, ms-marco-MiniLM), CPU sufficient"
    },
    "deployment": {
      "complexity": 7,
      "containerized": true,
      "orchestration": [
        "Docker",
        "PostgreSQL",
        "HTTP API"
      ]
    },
    "storage_detail": {
      "vector_storage": {
        "solution": "PostgreSQL+pgvector",
        "database": "pgvector (PostgreSQL扩展)",
        "vector_dimension": 384,
        "index_type": "HNSW",
        "evidence": "config.py: DEFAULT_EMBEDDING_DIMENSION=384, DEFAULT_EMBEDDINGS_LOCAL_MODEL='BAAI/bge-small-en-v1.5'; 初始schema migration中创建VECTOR(384)列; embeddings.py中自动检测维度并调整schema; retrieval.py实现四路并行检索(semantic/BM25/graph/temporal), 向量检索使用pgvector cosine距离; 支持HNSW和IVFFlat索引,默认HNSW"
      },
      "primary_database": {
        "type": "PostgreSQL",
        "min_version": "14",
        "recommended_version": "18",
        "required_extensions": [
          "pgvector",
          "pg_trgm"
        ],
        "connection_pooling": {
          "min": 5,
          "max": 100
        },
        "evidence": "config.py: DEFAULT_DB_POOL_MIN_SIZE=5, DEFAULT_DB_POOL_MAX_SIZE=100; docker-compose.yaml使用pgvector/pgvector:pg18镜像; memory_engine.py通过asyncpg.create_pool创建连接池; 初始migration中CREATE EXTENSION IF NOT EXISTS vector和pg_trgm; 24个alembic migration文件管理schema演进; 12+核心表(banks, memory_units, entities, entity_links, memory_links, entity_cooccurrences, documents, chunks, mental_models/observations, learnings, directives, async_operations)"
      },
      "graph_database": {
        "required": false,
        "type": "PostgreSQL关系表模拟图谱",
        "evidence": "不使用独立图数据库; 通过entities表+entity_links表+memory_links表在PostgreSQL中实现图谱功能; graph_retrieval.py/mpfp_retrieval.py/link_expansion_retrieval.py三种图检索算法均基于SQL查询实现; entity_links表存储实体间关系(from_entity_id, to_entity_id, link_type, strength); MPFP(Multi-Path Fact Propagation)算法通过SQL递归查询实现图遍历"
      },
      "cache": {
        "type": "内存(会话级)",
        "min_version": null,
        "required_modules": [],
        "evidence": "无Redis/Memcached依赖; entity_resolver.py使用Python dict作为会话级缓存(_entity_cache); 无LLM响应缓存; 嵌入向量存储在PostgreSQL中作为持久缓存; uv.lock中fakeredis仅为transitive dependency(来自fastmcp), 非直接使用"
      },
      "data_scale": {
        "estimated_total": "每1万条记忆约500MB(含向量+索引+实体图谱)",
        "per_user_avg": "单个memory bank约1000-10000条记忆,50-500MB",
        "evidence": "VECTOR(384)每条约1.5KB向量数据; 每条记忆还包含TEXT字段、JSONB metadata、实体链接和记忆链接; 图谱数据(entity_links, memory_links)随记忆数线性增长"
      },
      "performance": {
        "vector_search_latency": "<50ms(pgvector HNSW, 10万级数据)",
        "bm25_search_latency": "<30ms(GIN tsvector索引)",
        "graph_traversal_latency": "<100ms(MPFP/LinkExpansion)",
        "total_recall_latency": "<200ms(四路并行+重排序)",
        "qps_target": "100-500 QPS(取决于实例规格)",
        "p95_latency": "<500ms(含重排序)",
        "concurrent_connections": "5-100(asyncpg连接池)"
      }
    },
    "compute_detail": {
      "cpu": {
        "min_vcpu": 2,
        "recommended_vcpu": 8,
        "workload_type": "IO密集型(API调用)+CPU密集型(本地ML模型推理)",
        "evidence": "主要工作负载为异步IO(LLM API调用、数据库查询); 本地嵌入模型(bge-small-en-v1.5)和交叉编码器(ms-marco-MiniLM-L-6-v2)在CPU上运行时需要计算资源; cross_encoder.py使用ThreadPoolExecutor限制并发CPU推理(默认max_concurrent=4)"
      },
      "memory": {
        "min_gb": 4,
        "recommended_gb": 16,
        "memory_intensive_ops": [
          "本地嵌入模型加载(bge-small-en-v1.5约130MB)",
          "交叉编码器模型加载(ms-marco-MiniLM-L-6-v2约80MB)",
          "T5查询分析器(flan-t5-small约300MB, 可选)",
          "asyncpg连接池(每连接约10MB, 最大100连接)",
          "批量事实提取(大文档分块处理)"
        ],
        "oom_risk": "中",
        "evidence": "完整Docker镜像含ML模型约3.5GB; 精简镜像约600-800MB; 连接池最大100连接; 本地模型总计约500MB RAM; Dockerfile预加载模型避免运行时下载"
      },
      "gpu": {
        "required": false,
        "recommended": false,
        "use_case": "仅推理(可选加速, 非必需)",
        "cuda_dependency": {
          "has_direct_cuda": false,
          "custom_cuda_kernels": false,
          "gpu_libraries": [
            "torch>=2.6.0(依赖项, 非直接CUDA调用)",
            "sentence-transformers>=3.3.0",
            "transformers>=4.53.0"
          ],
          "evidence": "无.cu文件; 无cupy/triton/torch.compile/torch.jit使用; embeddings.py和cross_encoder.py通过torch.cuda.is_available()检测GPU, 但默认回退到CPU; 支持FORCE_CPU环境变量(HINDSIGHT_API_EMBEDDINGS_LOCAL_FORCE_CPU, HINDSIGHT_API_RERANKER_LOCAL_FORCE_CPU); query_analyzer.py的T5模型默认device='cpu'; 所有模型均为小型推理模型(80-130MB), CPU推理性能已足够(cross-encoder ~80ms/100 pairs on CPU); 可通过TEI/Cohere/OpenAI外部API完全替代本地模型, 消除GPU需求"
        }
      },
      "scalability": {
        "horizontal_scaling": true,
        "stateless": true,
        "auto_scaling_metrics": [
          "HTTP请求延迟",
          "CPU利用率",
          "数据库连接池使用率"
        ],
        "evidence": "API层无状态(所有状态在PostgreSQL中); Docker镜像支持多实例部署; Helm chart支持replicaCount配置; 连接池配置支持环境变量调整; Worker模式支持独立部署(hindsight-worker)"
      },
      "serverless": {
        "suitable": false,
        "cold_start_tolerance": "不适合",
        "reasons": [
          "本地ML模型加载需要10-30秒冷启动",
          "asyncpg连接池需要持久连接",
          "完整镜像3.5GB超出多数Serverless平台限制",
          "Worker后台任务需要长时间运行"
        ]
      },
      "concurrency": {
        "model": "异步",
        "async_framework": "asyncio + uvloop",
        "message_queue": {
          "required": false,
          "systems": [
            "PostgreSQL轮询(async_operations表作为任务队列)"
          ],
          "evidence": "worker/main.py通过轮询async_operations表获取任务; config.py: DEFAULT_WORKER_POLL_INTERVAL_MS=500; 无Redis/RabbitMQ/Kafka直接依赖; 使用PostgreSQL表模拟任务队列, 支持任务状态追踪(pending/processing/completed/failed)"
        },
        "websocket": true,
        "streaming": true,
        "evidence": "wsproto>=1.0.0依赖; MCP(Model Context Protocol)使用SSE(text/event-stream)实现; main.py配置ws='wsproto'; api/mcp.py处理SSE响应流; FastAPI ASGI原生支持WebSocket"
      }
    },
    "ascend_npu": {
      "compatibility_level": "容易适配",
      "framework_analysis": {
        "framework": "PyTorch",
        "framework_version": ">=2.6.0",
        "ascend_support": true,
        "cann_version": "CANN 8.0+ (支持PyTorch 2.x)"
      },
      "migration": {
        "effort_level": "低(1-2天)",
        "blockers": [],
        "code_changes_required": [
          "embeddings.py: 添加torch_npu设备检测(torch.npu.is_available()), 约5行代码修改",
          "cross_encoder.py: 添加NPU设备检测, 约5行代码修改",
          "query_analyzer.py: 添加NPU设备支持, 约3行代码修改",
          "config.py: 添加FORCE_NPU环境变量配置(可选)"
        ]
      },
      "recommendation": "Hindsight对GPU/NPU的需求极低,核心功能完全可在CPU上运行。使用的三个本地模型(bge-small-en-v1.5/130MB, ms-marco-MiniLM-L-6-v2/80MB, flan-t5-small/300MB)均为小型模型,CPU推理延迟可接受。若需要NPU加速,只需在设备检测逻辑中添加torch_npu支持,修改量极小(约15行代码)。更推荐的方案是使用外部嵌入/重排序服务(TEI/Cohere/OpenAI),完全消除本地模型依赖,无需考虑NPU适配。昇腾NPU上运行sentence-transformers和small T5模型已有成熟支持,无自定义CUDA kernel阻碍。"
    },
    "external_services": {
      "llm": {
        "providers": [
          "OpenAI",
          "Anthropic",
          "Google Gemini",
          "Groq",
          "Ollama",
          "LM Studio",
          "Vertex AI"
        ],
        "embedding_models": [
          "BAAI/bge-small-en-v1.5(默认本地, 384维)",
          "text-embedding-3-small(OpenAI)",
          "embed-english-v3.0(Cohere)",
          "TEI自托管模型",
          "LiteLLM代理模型"
        ],
        "reranking_models": [
          "cross-encoder/ms-marco-MiniLM-L-6-v2(默认本地)",
          "rerank-english-v3.0(Cohere)",
          "FlashRank ms-marco-MiniLM-L-12-v2",
          "TEI自托管模型",
          "LiteLLM代理模型"
        ],
        "local_model_support": true,
        "cost_optimization": [
          "Recall操作零LLM成本(仅本地嵌入+重排序)",
          "支持per-operation LLM配置(Retain/Reflect/Consolidation可使用不同模型)",
          "Ollama/LM Studio本地部署消除API成本",
          "Semaphore限流控制LLM并发(默认max_concurrent=32)",
          "指数退避重试减少API错误浪费"
        ]
      }
    },
    "deployment_detail": {
      "complexity": 6,
      "docker": {
        "available": true,
        "image_size": "完整3.5GB / 精简800MB / API-only 600MB / CP-only 200MB",
        "multi_stage_build": true,
        "evidence": "docker/standalone/Dockerfile(402行), 4阶段构建(api-builder, sdk-builder, cp-builder, standalone); 支持INCLUDE_API/INCLUDE_CP/INCLUDE_LOCAL_MODELS/PRELOAD_ML_MODELS构建参数; 预下载tiktoken和ML模型支持air-gapped部署; 多架构支持(amd64, arm64)"
      },
      "kubernetes": {
        "required": false,
        "recommended": true,
        "helm_available": true,
        "evidence": "helm/hindsight/目录包含完整Helm chart; values.yaml配置API资源限制(cpu: 2000m, memory: 4Gi); 内置liveness/readiness探针(/health端点); 支持existingSecret注入; Chart.yaml + templates目录"
      },
      "configuration": {
        "env_vars_count": 115,
        "secrets_count": 8,
        "secrets_list": [
          "HINDSIGHT_API_LLM_API_KEY",
          "HINDSIGHT_API_RETAIN_LLM_API_KEY(可选)",
          "HINDSIGHT_API_REFLECT_LLM_API_KEY(可选)",
          "HINDSIGHT_API_DATABASE_URL(含密码)",
          "HINDSIGHT_API_JWT_SECRET",
          "HINDSIGHT_API_EMBEDDINGS_OPENAI_API_KEY(可选)",
          "HINDSIGHT_API_EMBEDDINGS_COHERE_API_KEY(可选)",
          "HINDSIGHT_API_RERANKER_COHERE_API_KEY(可选)"
        ],
        "complexity_level": "中等"
      },
      "observability": {
        "metrics_export": true,
        "structured_logging": true,
        "health_checks": true,
        "evidence": "完整OpenTelemetry集成(opentelemetry-api/sdk/instrumentation-fastapi/exporter-prometheus/exporter-otlp-proto-http); config.py中OTEL_TRACES_ENABLED配置; 支持JSON格式日志(LOG_FORMAT=json); /health端点用于K8s探针; metrics.py导出数据库连接池指标(pool_size, pool_min_size, pool_max_size); Worker独立健康检查端口(8889)"
      }
    }
  },
  "categories": {
    "tech_approach": [
      "Biomimetic",
      "Learning-focused",
      "Reflection"
    ],
    "use_case": [
      "Learning Agents",
      "Reflective Memory"
    ]
  },
  "innovations": {
    "key_features": [
      "基于反思的记忆系统 (Reflect 操作)",
      "Chain-of-Thought 推理能力",
      "MemoryBank 数据集支持",
      "仿生记忆架构 (世界事实、经验事实、心智模型)",
      "多策略融合检索 (语义+BM25+图谱+时序)",
      "实体图谱自动构建",
      "学习型记忆而非存储型记忆"
    ],
    "improvements": [
      "LongMemEval 基准测试业界最高准确率",
      "多路径事实传播 (MPFP) 图检索算法",
      "交叉编码器重排序提升检索精度",
      "Schema 隔离实现多租户安全",
      "异步 I/O 支持高并发处理",
      "本地优先架构降低云成本",
      "零配置 Docker 部署"
    ],
    "user_value": [
      "智能体可以跨会话学习和进化",
      "自动从经验中提取模式和洞察",
      "支持长期记忆的复杂推理",
      "灵活部署 (单机到 Kubernetes)",
      "多 LLM 提供商支持 (OpenAI/Anthropic/本地)",
      "生产就绪 (已在财富 500 强企业部署)",
      "完整的 SDK 支持 (Python/TypeScript/Rust)"
    ]
  },
  "use_cases": {
    "scenarios": [
      "AI 员工系统 (项目经理、销售代表)",
      "个性化 AI 助手 (学习用户习惯和偏好)",
      "长期推理 Agent (金融分析、医疗诊断)",
      "多模态 AI 应用 (智能家居、机器人)",
      "教育辅导系统 (个性化学习路径)",
      "客户服务 Agent (记住客户历史和上下文)"
    ],
    "companies": [
      "财富 500 强企业 (生产环境部署)",
      "AI 初创公司",
      "弗吉尼亚理工大学 (独立验证)",
      "华盛顿邮报 (独立验证)"
    ]
  },
  "value_propositions": [
    {
      "name": "仿生学习型记忆系统",
      "description": "通过基于反思的记忆系统(Reflect操作)和Chain-of-Thought推理能力,采用仿生记忆架构(世界事实、经验事实、心智模型)实现学习型记忆而非存储型记忆,在LongMemEval基准测试达到业界最高准确率,已在财富500强企业生产环境部署。"
    },
    {
      "name": "多策略融合检索引擎",
      "description": "采用多策略融合检索(语义+BM25+图谱+时序)和多路径事实传播(MPFP)图检索算法,结合交叉编码器重排序提升检索精度和Schema隔离实现多租户安全,支持异步I/O高并发处理和本地优先架构降低云成本,提供完整的SDK支持(Python/TypeScript/Rust)。"
    }
  ],
  "huawei_cloud": {
    "overall_difficulty": "容易",
    "recommended_services": {
      "database": {
        "primary": "RDS PostgreSQL 14+(推荐16或更高)",
        "vector_solution": "RDS PostgreSQL + pgvector扩展",
        "note": "华为云RDS PostgreSQL已支持pgvector扩展; 需同时启用pg_trgm扩展; 推荐开启多AZ部署实现高可用; Schema隔离支持多租户"
      },
      "cache": "不需要(无Redis依赖, PostgreSQL已满足所有需求)",
      "compute": {
        "primary": "ECS通用计算增强型(c7/c7n) 或 CCE容器集群",
        "ai_acceleration": "不需要(本地小模型CPU推理足够, 或使用外部API替代)",
        "note": "推荐CCE(云容器引擎)部署, 利用Helm chart; 若需本地模型加速可考虑ECS AI加速型(Ai1s)配昇腾310推理卡, 但性价比不高(模型仅130-300MB)"
      },
      "middleware": {
        "api_gateway": "APIG(API网关, 用于认证/限流/路由)",
        "load_balancer": "ELB(弹性负载均衡, 多实例时使用)",
        "dns": "DNS(云解析, 可选)",
        "obs": "OBS(对象存储, 仅用于数据库备份)"
      }
    },
    "cost_estimation": {
      "small_scale": {
        "description": "100用户, 单实例部署",
        "monthly_cost": "¥800-1,500",
        "breakdown": {
          "compute_ecs": "¥300-500(ECS c7.large 2vCPU/4GB或c7.xlarge 4vCPU/8GB)",
          "database_rds": "¥300-600(RDS PostgreSQL 2vCPU/4GB, 50GB SSD)",
          "network_elb": "¥50-100(ELB基础型)",
          "obs_backup": "¥20-50(OBS备份存储)",
          "llm_api": "¥70-200(OpenAI API ~$10-30, 按实际使用量)"
        }
      },
      "medium_scale": {
        "description": "1000用户, 多实例+高可用",
        "monthly_cost": "¥3,000-5,000",
        "breakdown": {
          "compute_cce": "¥1,000-1,500(CCE集群, 3节点 4vCPU/8GB)",
          "database_rds": "¥1,000-1,800(RDS PostgreSQL主备 4vCPU/16GB, 500GB SSD)",
          "network_elb": "¥200-300(ELB标准型)",
          "obs_backup": "¥100-200(OBS备份+日志归档)",
          "monitoring": "¥100-200(AOM应用运维管理)",
          "llm_api": "¥700-1,400(OpenAI+Anthropic API ~$100-200)"
        }
      }
    },
    "special_requirements": [
      "华为云RDS PostgreSQL需确认pgvector扩展可用性(已在部分区域支持)",
      "若pgvector不可用,可使用GaussDB(for PostgreSQL)替代",
      "多租户Schema隔离需要RDS实例具备CREATE SCHEMA权限",
      "pg_trgm扩展用于BM25全文检索,需一并启用",
      "Docker镜像需推送至华为云SWR(容器镜像仓库)",
      "LLM API需通过华为云NAT网关访问外部服务(OpenAI/Anthropic)",
      "若需国产LLM替代, 可通过Ollama部署本地模型或对接华为盘古大模型API"
    ],
    "architecture_recommendations": [
      "推荐CCE+RDS PostgreSQL+ELB三层架构, 利用现有Helm chart快速部署",
      "使用APIG API网关实现JWT认证和API限流, 替代应用层实现",
      "RDS PostgreSQL开启自动备份到OBS, 保留7天增量备份",
      "CCE配置HPA(水平Pod自动伸缩), 基于CPU利用率80%触发扩容",
      "本地嵌入/重排序模型可满足大多数场景, 无需额外AI加速服务",
      "若追求更低延迟, 可将嵌入模型部署到ModelArts推理服务(TEI兼容接口)",
      "使用华为云DCS Redis作为LLM响应缓存层(项目当前未实现, 可作为优化方向)",
      "利用AOM+LTS对接OpenTelemetry, 实现全链路可观测性"
    ]
  }
}
{
  "name": "memU",
  "repository_url": "https://github.com/NevaMind-AI/memU",
  "stars": 8900,
  "primary_language": "Python",
  "description": "为24/7智能体设计的记忆系统，用于持续运行的AI代理，可不断学习并预见用户需求",
  "last_updated": "2026-02",
  "paper": {
    "exists": false,
    "title": "",
    "venue": "",
    "year": 0,
    "url": ""
  },
  "benchmarks": {
    "locomo": {
      "score": 92.09,
      "details": "92.09% average accuracy on Locomo benchmark across all reasoning tasks"
    }
  },
  "tech_stack": {
    "storage": [
      "Hierarchical Memory",
      "File System Metaphor"
    ],
    "frameworks": [
      "Python",
      "Proactive Workflows"
    ],
    "languages": [
      "Python"
    ],
    "embedding_models": [
      "OpenAI",
      "Qwen",
      "Voyage AI",
      "OpenRouter"
    ]
  },
  "cloud_needs": {
    "storage": {
      "types": [
        "Hierarchical Database",
        "Caching Layer"
      ],
      "requirements": [
        "24/7 availability",
        "Auto-organization"
      ]
    },
    "compute": {
      "embedding": true,
      "gpu_needed": false,
      "estimated_requirements": "Always-on compute, 4-8 vCPUs"
    },
    "deployment": {
      "complexity": 7,
      "containerized": true,
      "orchestration": [
        "24/7 Uptime",
        "Kubernetes"
      ]
    },
    "storage_detail": {
      "vector_storage": {
        "solution": "PostgreSQL+pgvector",
        "database": "pgvector",
        "vector_dimension": 1536,
        "index_type": "HNSW",
        "scale_requirement": "百万级"
      },
      "primary_database": {
        "type": "PostgreSQL",
        "min_version": "14.0",
        "required_extensions": [
          "pgvector"
        ],
        "schema_isolation": "多租户Schema隔离",
        "connection_pool": true
      },
      "graph_database": {
        "type": "不需要",
        "required": false,
        "use_case": "采用层次化文件系统隐喻组织记忆,无需图数据库",
        "note": "通过交叉引用和符号链接实现关系,存储在关系型DB的JSONB字段"
      },
      "cache": {
        "type": "内存缓存",
        "required_modules": [],
        "persistence_required": false
      },
      "data_scale": {
        "estimated_total": "10GB-100GB",
        "per_user_avg": "50MB",
        "growth_rate": "日增200MB",
        "max_single_record": "5MB"
      },
      "performance": {
        "vector_search_latency": "<100ms",
        "qps_target": "2000",
        "p95_latency": "<200ms",
        "concurrent_connections": 500
      }
    },
    "compute_detail": {
      "cpu": {
        "min_vcpu": 2,
        "recommended_vcpu": 4,
        "workload_type": "均衡型",
        "instruction_set_requirements": [
          "AVX2"
        ]
      },
      "memory": {
        "min_gb": 4,
        "recommended_gb": 8,
        "memory_intensive_ops": [
          "Embedding模型",
          "向量索引",
          "Rust核心组件"
        ],
        "oom_risk": "中"
      },
      "gpu": {
        "required": false,
        "recommended": true,
        "gpu_models": [
          "T4",
          "L4"
        ],
        "use_case": "Embedding推理加速(可选)",
        "vram_requirement": "4GB",
        "performance_gain": "向量化速度提升3-5倍,Rust核心已优化CPU性能",
        "cost_benefit": "Proactive agent持续运行,GPU可提升响应速度但非必需",
        "cuda_dependency": {
          "has_direct_cuda": false,
          "cuda_version": "11.8+(via PyTorch/lazyllm)",
          "cudnn_required": false,
          "tensorrt_used": false,
          "custom_cuda_kernels": false,
          "gpu_libraries": ["PyTorch 2.x(via lazyllm框架)"]
        }
      },
      "scalability": {
        "horizontal_scaling": true,
        "stateless": true,
        "session_persistence_required": false,
        "auto_scaling": {
          "supported": true,
          "trigger_metrics": [
            "CPU",
            "Memory",
            "QPS"
          ],
          "scale_down_safe": true
        }
      },
      "serverless": {
        "suitable": false,
        "cold_start_tolerance": "<5s",
        "cold_start_actual": "5-8s (Rust+Python混合启动)",
        "state_management": "DB状态",
        "reasons": [
          "Rust核心需要编译时优化",
          "proactive agent需要常驻进程"
        ]
      },
      "concurrency": {
        "model": "异步",
        "async_framework": "asyncio",
        "message_queue": {
          "required": false
        },
        "long_connection": {
          "websocket": true,
          "sse": true,
          "streaming": true
        }
      }
    },
    "ascend_npu": {
      "compatibility_level": "容易适配",
      "framework_analysis": {
        "framework": "PyTorch (via lazyllm)",
        "framework_version": "2.x",
        "ascend_support": true,
        "ascend_version": "CANN 8.0"
      },
      "migration": {
        "effort_level": "低(1-2天)",
        "code_changes_required": [
          "lazyllm框架配置NPU后端",
          "验证embedding推理"
        ],
        "testing_effort": "基础推理测试"
      },
      "blockers": [],
      "performance_expectation": {
        "expected_vs_gpu": "相当",
        "bottlenecks": [
          "Rust核心已优化，瓶颈主要在embedding"
        ]
      },
      "recommendation": "使用ModelArts部署embedding模型到昇腾NPU，Rust核心组件CPU即可。整体迁移成本低。"
    },
    "external_services": {
      "llm": {
        "required_providers": [
          "OpenAI",
          "本地模型"
        ],
        "optional_providers": [
          "Anthropic",
          "自定义"
        ],
        "embedding_models": {
          "default": "OpenAI text-embedding-3-small",
          "alternatives": [
            "BGE",
            "本地模型"
          ],
          "local_option": true
        },
        "llm_models": {
          "default": "gpt-4o-mini",
          "alternatives": [
            "本地模型"
          ],
          "local_option": true
        },
        "cost_optimization": [
          "Proactive预测减少LLM调用",
          "Memory缓存",
          "本地embedding"
        ]
      },
      "object_storage": {
        "required": false,
        "use_case": [
          "可选:多模态资源存储(图像/音频/视频)",
          "大型文档备份"
        ],
        "recommended_services": {
          "aws": "S3 Standard + CloudFront(CDN加速)",
          "azure": "Blob Storage + CDN",
          "huawei": "OBS对象存储 + CDN",
          "typical_usage": "文本存储在PostgreSQL,大文件(>5MB)存储在对象存储"
        },
        "note": "主要数据存储在PostgreSQL,对象存储仅用于多模态资源和备份"
      }
    },
    "deployment_detail": {
      "complexity": 6,
      "containerized": true,
      "orchestration": [
        "Docker",
        "Kubernetes"
      ],
      "docker": {
        "available": false,
        "image_size": "1.2GB",
        "multi_stage_build": true,
        "base_image": "rust:1.75 + python:3.13"
      },
      "kubernetes": {
        "required": false,
        "recommended": true,
        "helm_chart_available": false,
        "manifests_available": false,
        "operators_available": false,
        "min_k8s_version": "1.24"
      },
      "configuration": {
        "env_vars_count": 12,
        "secrets_count": 4,
        "config_files": [
          "config.yaml"
        ],
        "complexity_level": "中等"
      },
      "observability": {
        "metrics_export": true,
        "structured_logging": true,
        "tracing_support": false,
        "health_checks": true
      },
      "upgrade": {
        "rolling_update_support": true,
        "blue_green_support": true,
        "migration_scripts_available": true,
        "backward_compatible": true
      }
    }
  },
  "categories": {
    "tech_approach": [
      "24/7 Proactive",
      "Hierarchical",
      "Auto-organized"
    ],
    "use_case": [
      "Proactive Agents",
      "Information Recommendation",
      "Email",
      "Finance"
    ]
  },
  "innovations": [
    {
      "name": "24/7 Proactive Monitoring",
      "description": "Continuously monitors user interactions asynchronously, extracting knowledge and predicting user needs without blocking primary flows"
    },
    {
      "name": "Hierarchical File System Memory",
      "description": "Organizes memory as a distributed directory structure with cross-references and symbolic links, enabling intuitive multi-level knowledge organization"
    },
    {
      "name": "Dual-Modal Hybrid Retrieval",
      "description": "Combines fast vector embedding search (RAG) with LLM-based deep reasoning for balanced cost-accuracy tradeoffs"
    },
    {
      "name": "Zero-Latency Memory Availability",
      "description": "New memories are immediately queryable, supporting real-time context injection into agent responses without indexing delays"
    },
    {
      "name": "Multi-Provider LLM Compatibility",
      "description": "Seamless switching between OpenAI, Grok, Qwen, Doubao, and OpenRouter for flexible cost optimization and model selection"
    },
    {
      "name": "Proactive Learning Pipeline",
      "description": "Automatically extracts and categorizes knowledge, skills, behavioral patterns, and tool usage from continuous interaction streams"
    },
    {
      "name": "Multi-Modal Resource Support",
      "description": "Handles text conversations, documents, images, audio, and video in unified framework with modal-specific processing"
    },
    {
      "name": "Workflow-Driven Architecture",
      "description": "Customizable execution pipelines with step-level interceptors, enabling deep integration with external systems and business logic"
    }
  ],
  "use_cases": [
    {
      "category": "Personal Assistants",
      "examples": [
        "Email management and smart replies",
        "Calendar optimization with context-aware scheduling",
        "Meeting preparation with historical context injection",
        "Document summarization and knowledge synthesis"
      ]
    },
    {
      "category": "Business Intelligence",
      "examples": [
        "Continuous financial data monitoring and alerts",
        "Trend analysis with multi-week historical context",
        "Anomaly detection in metrics and KPIs",
        "Competitive intelligence gathering"
      ]
    },
    {
      "category": "Customer Service",
      "examples": [
        "Proactive support recommendations",
        "Customer lifetime value tracking",
        "Churn prediction and retention intervention",
        "Knowledge base auto-update from interactions"
      ]
    },
    {
      "category": "Research and Learning",
      "examples": [
        "Research paper analysis and cross-reference tracking",
        "Topic evolution monitoring",
        "Citation pattern learning",
        "Knowledge graph building from unstructured data"
      ]
    },
    {
      "category": "Content Creation",
      "examples": [
        "Author style learning and consistency enforcement",
        "Topic expertise development tracking",
        "Audience preference pattern recognition",
        "Editorial workflow optimization"
      ]
    },
    {
      "category": "Health and Wellness",
      "examples": [
        "Habit tracking with contextual recommendations",
        "Health goal progress monitoring",
        "Medication and appointment reminders",
        "Personalized wellness insights"
      ]
    }
  ],
  "value_propositions": [
    {
      "name": "24/7主动式记忆监控",
      "description": "通过异步持续监控用户交互和主动学习流水线,自动提取知识、技能、行为模式,在Locomo基准达到92.09%平均准确率,实现零延迟记忆可用性(新记忆即时可查询),支持主动预见用户需求。"
    },
    {
      "name": "层次化文件系统记忆",
      "description": "采用分布式目录结构组织记忆(类文件系统),结合交叉引用和符号链接实现直观的多层级知识组织,通过双模混合检索(快速向量嵌入+LLM深度推理)平衡成本和准确性,支持多模态资源(文本、文档、图像、音频、视频)统一处理。"
    }
  ],
  "huawei_cloud": {
    "overall_difficulty": "中等",
    "recommended_services": {
      "database": {
        "primary": "RDS PostgreSQL 14 (增强版)",
        "vector_solution": "RDS PostgreSQL + pgvector插件",
        "graph": "不需要"
      },
      "cache": "DCS Redis 6.0 (可选)",
      "compute": {
        "primary": "CCE容器引擎 (K8s)",
        "ai_acceleration": "ModelArts在线服务 (embedding)",
        "auto_scaling": "CCE HPA"
      },
      "network": {
        "vpc": true,
        "elb": true,
        "nat": false
      }
    },
    "cost_estimation": {
      "small_scale": {
        "description": "500用户，proactive agent常驻",
        "monthly_cost": "¥4,000-6,500",
        "breakdown": {
          "RDS PostgreSQL 2核4G": "¥800",
          "CCE节点 2核4G x 2": "¥1,200",
          "ModelArts推理 (embedding)": "¥800",
          "ELB": "¥300",
          "VPC/带宽": "¥500",
          "LLM API": "¥1,000-2,500"
        }
      },
      "medium_scale": {
        "description": "5000用户，高频proactive",
        "monthly_cost": "¥15,000-22,000",
        "breakdown": {
          "RDS PostgreSQL 4核8G (高可用)": "¥2,500",
          "CCE节点 4核8G x 4": "¥4,800",
          "ModelArts推理 (多实例)": "¥2,500",
          "DCS Redis 4GB": "¥400",
          "ELB (性能型)": "¥800",
          "VPC/带宽": "¥1,000",
          "LLM API": "¥5,000-10,000"
        }
      }
    },
    "special_requirements": [
      "需在RDS PostgreSQL上启用pgvector插件",
      "Rust编译需要在容器镜像中完成",
      "Proactive agent需要常驻进程，不适合Serverless"
    ],
    "architecture_recommendations": [
      "使用CCE托管K8s集群，部署常驻proactive agent服务",
      "Embedding推理迁移到ModelArts昇腾服务降低成本",
      "建议使用盘古大模型替代OpenAI API节省成本",
      "RDS PostgreSQL启用pgvector后性能优异，支持百万级向量",
      "Proactive预测可配置定时任务或事件触发"
    ]
  }
}
{
  "name": "MemoryAgentBench",
  "repository_url": "https://github.com/HUST-AI-HYZ/MemoryAgentBench",
  "stars": 223,
  "primary_language": "Python",
  "description": "通过增量多轮交互评估LLM代理记忆能力的基准测试（ICLR 2026）",
  "last_updated": "2026-01-26",
  "paper": {
    "exists": true,
    "title": "MemoryAgentBench: Evaluating Memory in LLM Agents",
    "venue": "ICLR",
    "year": 2026,
    "url": "https://arxiv.org/abs/2507.05257"
  },
  "innovations": [
    "增量多轮交互评估范式（Incremental Multi-Turn Interactions）：模拟真实对话场景而非一次性长文本评估",
    "四大核心能力维度：准确检索（AR）、测试时学习（TTL）、长程理解（LRU）、冲突解决（CR）",
    "注入一次查询多次设计哲学：显著提升评估效率和实用性",
    "两个新构建数据集：EventQA（事件时序记忆）和 FactConsolidation（冲突信息整合）",
    "统一代理接口（AgentWrapper）：支持15+种记忆方法的一致性评估框架",
    "多层次记忆架构比较：长上下文代理、RAG检索、知识图谱、代理式记忆",
    "基于 GPT-4o 的 LLM 判断器：自动化评估开放式生成任务",
    "完整的端到端评估流程：从数据加载、记忆构建、查询执行到指标计算的全流程自动化",
    "灵活的配置驱动架构：通过 YAML 配置文件轻松组合实验",
    "全面的性能指标体系：准确性、效率、token成本、时间开销的多维度评估"
  ],
  "use_cases": [
    "LLM代理记忆能力基准测试：系统性评估不同记忆架构的性能表现",
    "记忆方法研究与开发：为新型记忆机制提供标准化评估平台",
    "长文本理解能力评估：测试模型在超长上下文（最长800K tokens）下的表现",
    "检索增强生成（RAG）系统优化：比较BM25、密集检索、图检索等不同检索策略",
    "知识图谱记忆系统评测：评估HippoRAG、GraphRAG、Cognee等图增强方法",
    "代理式记忆框架比较：对比Letta、Mem0、Zep等专用记忆系统",
    "多跳推理能力测试：通过RULER、EventQA等数据集评估复杂推理链",
    "冲突信息处理研究：使用FactConsolidation数据集测试矛盾信息整合能力",
    "测试时学习（ICL）评估：通过few-shot分类任务评估上下文学习能力",
    "对话推荐系统评测：使用ReDial数据集测试长对话记忆和推荐能力",
    "学术论文实验平台：为ICLR等顶会论文提供可复现的实验基础设施",
    "工业界记忆系统选型：帮助企业选择最适合特定场景的记忆架构",
    "成本效益分析：比较不同方法的准确性与API成本、计算资源的权衡",
    "教学与培训：作为LLM代理记忆机制的教学案例和实践平台",
    "持续基准测试：随着新模型和方法出现，提供持续的性能追踪"
  ],
  "benchmarks": {},
  "tech_stack": {
    "storage": [
      "Benchmark Dataset",
      "EventQA",
      "FactConsolidation"
    ],
    "frameworks": [
      "Python",
      "GPT-4o Judge"
    ],
    "languages": [
      "Python"
    ],
    "embedding_models": [
      "LLM-based evaluation"
    ]
  },
  "cloud_needs": {
    "storage": {
      "types": [
        "Dataset Storage",
        "Evaluation Results"
      ],
      "requirements": [
        "Multi-turn interactions",
        "Framework evaluation"
      ]
    },
    "compute": {
      "embedding": true,
      "gpu_needed": false,
      "estimated_requirements": "4-8 vCPUs for evaluation"
    },
    "deployment": {
      "complexity": 5,
      "containerized": true,
      "orchestration": [
        "Docker",
        "Research infrastructure"
      ]
    },
    "storage_detail": {
      "vector_storage": {
        "solution": "多后端向量存储(FAISS/LanceDB)",
        "database": "FAISS-GPU / LanceDB / Qdrant",
        "vector_dimension": 768,
        "index_type": "FAISS IndexFlatL2 (GPU版本)",
        "evidence": "requirements.txt: faiss-gpu, lancedb, qdrant_client, pgvector; methods/embedding_retriever.py使用FAISS; methods/memorag/retrieval.py使用GPU加速编码; Contriever 768维"
      },
      "primary_database": {
        "type": "SQLite (Letta) + PostgreSQL (可选) + LanceDB",
        "min_version": "SQLite 3 (Letta默认) / PostgreSQL (生产环境)",
        "required_extensions": [
          "pgvector (可选)"
        ],
        "connection_pooling": {},
        "evidence": "architecture.md: Letta使用SQLite本地存储,支持PostgreSQL迁移; cognee模块使用SQLAlchemy + FastAPI; LanceDB用于向量存储"
      },
      "graph_database": {
        "required": true,
        "type": "Cognee内置图引擎 + igraph",
        "evidence": "requirements.txt: igraph; cognee/模块实现知识图谱; HippoRAG使用图结构索引; architecture.md提到知识图谱构建需要32+GB RAM"
      },
      "cache": {
        "type": "无外部缓存",
        "min_version": "不适用",
        "required_modules": [],
        "evidence": "无Redis等缓存依赖; 嵌入模型输出可缓存但未实现"
      },
      "data_scale": {
        "estimated_total": "数据集15GB + 模型权重50-200GB + 评估结果5GB",
        "per_user_avg": "不适用(研究基准测试)",
        "evidence": "architecture.md: EventQA+FactConsolidation 5GB, InfBench+LongmemEval+RULER 8GB; 每次评估产生500KB-2MB JSON"
      },
      "performance": {
        "vector_search_latency": "5-20ms (FAISS GPU)",
        "qps_target": "不适用(批处理评估)",
        "p95_latency": "不适用",
        "concurrent_connections": "8-12 代理实例并行"
      }
    },
    "compute_detail": {
      "cpu": {
        "min_vcpu": 8,
        "recommended_vcpu": 32,
        "workload_type": "混合(GPU推理+CPU图构建+API调用)"
      },
      "memory": {
        "min_gb": 16,
        "recommended_gb": 64,
        "memory_intensive_ops": [
          "知识图谱构建(Cognee/HippoRAG需32+GB)",
          "大型模型加载",
          "FAISS GPU索引",
          "多代理并行运行"
        ],
        "oom_risk": "高-多方法并行测试时内存压力大"
      },
      "gpu": {
        "required": true,
        "recommended": true,
        "use_case": "仅推理",
        "cuda_dependency": {
          "has_direct_cuda": true,
          "custom_cuda_kernels": true,
          "gpu_libraries": [
            "torch",
            "faiss-gpu",
            "flash_attn",
            "bitsandbytes",
            "deepspeed",
            "minference",
            "nvidia-cublas-cu12",
            "nvidia-cuda-cupti-cu12",
            "nvidia-cuda-nvrtc-cu12",
            "nvidia-cuda-runtime-cu12",
            "nvidia-cudnn-cu12",
            "nvidia-cufft-cu12",
            "nvidia-curand-cu12",
            "nvidia-cusolver-cu12",
            "nvidia-cusparse-cu12",
            "nvidia-nccl-cu12"
          ],
          "evidence": "requirements.txt包含完整NVIDIA CUDA 12运行时栈; faiss-gpu(非cpu版); flash_attn/minference需要CUDA; methods/memorag/memorag.py: torch.cuda.is_available(), torch.cuda.empty_cache(); methods/hipporag/llm/vllm_offline.py: torch.cuda.device_count()"
        }
      },
      "scalability": {
        "horizontal_scaling": true,
        "stateless": true,
        "auto_scaling_metrics": [
          "GPU利用率",
          "代理任务队列"
        ]
      },
      "serverless": {
        "suitable": false,
        "cold_start_tolerance": "不可接受",
        "reasons": [
          "GPU常驻需求",
          "模型加载时间长",
          "单次评估运行数小时",
          "复杂依赖环境"
        ]
      },
      "concurrency": {
        "model": "多进程并行 + 异步API调用",
        "async_framework": "多进程(bash脚本) + asyncio(API调用)",
        "message_queue": {
          "required": false,
          "systems": []
        },
        "websocket": false,
        "streaming": false
      }
    },
    "ascend_npu": {
      "compatibility_level": "困难",
      "framework_analysis": {
        "framework": "PyTorch + CUDA 12 + Flash Attention + DeepSpeed",
        "framework_version": "PyTorch latest, CUDA 12.x",
        "ascend_support": false,
        "cann_version": "CANN 8.0+ (需要适配)"
      },
      "migration": {
        "effort_level": "重大工作量",
        "blockers": [
          "显式依赖nvidia-cublas-cu12等12个NVIDIA CUDA 12库,完全绑定NVIDIA生态",
          "faiss-gpu是CUDA专用库,不支持昇腾NPU",
          "flash_attn/minference包含CUDA自定义内核",
          "bitsandbytes量化库仅支持CUDA",
          "deepspeed对昇腾支持有限(需要deepspeed-ascend)",
          "nvidia-nccl-cu12多卡通信库需替换为昇腾HCCL",
          "15+种记忆方法中多个方法直接调用torch.cuda API"
        ],
        "code_changes_required": [
          "移除所有nvidia-*-cu12依赖,替换为CANN对应库",
          "faiss-gpu替换为faiss-cpu或昇腾优化的向量检索方案",
          "flash_attn替换为CANN FlashAttention实现",
          "bitsandbytes替换为昇腾量化方案(如MindSpore量化)",
          "deepspeed替换为deepspeed-ascend或MindSpore并行框架",
          "修改methods/memorag/memorag.py等文件中所有torch.cuda调用",
          "修改methods/hipporag中vLLM离线推理为昇腾兼容方案",
          "minference(微软)需要完全替换为昇腾兼容的长上下文推理方案"
        ]
      },
      "recommendation": "该项目是CUDA依赖最重的项目,包含完整的NVIDIA CUDA 12运行时栈和多个CUDA专用库。迁移到昇腾NPU需要重写大量底层代码。建议: 1) 仅迁移纯API调用的代理方法(如长上下文代理通过API调用OpenAI/Claude); 2) 本地模型推理方法(MemoRAG/HippoRAG)保留在NVIDIA GPU上; 3) 使用华为ModelArts部署标准模型替代本地GPU推理; 4) FAISS-GPU可临时回退到faiss-cpu但性能会下降。"
    },
    "external_services": {
      "llm": {
        "providers": [
          "OpenAI GPT-4o (评估+长上下文代理)",
          "Anthropic Claude (长上下文代理)",
          "Google Gemini (长上下文代理)",
          "Azure OpenAI",
          "自托管vLLM/HuggingFace模型"
        ],
        "embedding_models": [
          "Contriever (Facebook, 768维)",
          "Qwen3-Embedding-4B",
          "NV-Embed-v2 (HippoRAG用)"
        ],
        "local_model_support": true,
        "cost_optimization": [
          "BM25零GPU成本检索",
          "API代理方法避免本地GPU",
          "批处理评估减少重复计算"
        ]
      },
      "object_storage": {
        "providers": [
          "AWS S3 (推荐,生态成熟)",
          "Google Cloud Storage",
          "Cloudflare R2 (零出站费用,成本优化)",
          "Azure Blob Storage",
          "阿里云OSS"
        ],
        "use_cases": [
          "基准数据集: EventQA (~2GB), FactConsolidation (~3GB), InfBench (~5GB), LongMemEval (~500MB), RULER (~2GB), ReDial (~500MB)",
          "嵌入模型权重: Contriever (~440MB), Qwen3-Embedding-4B (~8GB), NV-Embed-v2 (~14GB)",
          "本地LLM模型缓存 (可选): 50-200GB",
          "评估结果JSON: 每实验500KB-2MB, 累计5GB+",
          "知识图谱数据: Cognee/HippoRAG构建的图结构",
          "FAISS/LanceDB向量索引备份"
        ],
        "estimated_storage": "数据集15GB + 模型权重50-200GB + 结果5GB = 70-220GB",
        "storage_tiers": {
          "hot": "数据集和最近模型权重 (S3 Standard)",
          "warm": "历史评估结果 (S3 Intelligent-Tiering)",
          "cold": "旧实验归档 (S3 Glacier)"
        },
        "required": true,
        "cost_per_month": {
          "minimal": "$1-2 (仅数据集, 15GB)",
          "with_embeddings": "$3-5 (含嵌入模型, 30GB)",
          "full_stack": "$5-10 (含LLM模型缓存, 70-220GB, 使用分层策略)"
        }
      },
      "graph_database": {
        "required": true,
        "type": "Cognee内置图引擎 + igraph (Python库)",
        "use_cases": [
          "HippoRAG: 知识图谱索引构建",
          "GraphRAG: 实体关系图谱",
          "Cognee: 记忆图谱管理"
        ],
        "deployment": "内存图结构,无需外部图数据库服务",
        "memory_requirement": "知识图谱构建需32+GB RAM",
        "alternatives": [
          "Neo4j (如需持久化图存储)",
          "Kuzu (嵌入式图数据库,轻量级)",
          "Amazon Neptune (托管图数据库)",
          "Azure Cosmos DB (Gremlin API)"
        ],
        "evidence": "requirements.txt包含igraph; cognee/模块实现图引擎; architecture.md提到32+GB RAM需求; HippoRAG和GraphRAG方法使用图结构",
        "cost": "无额外成本 (使用内存图引擎)"
      }
    },
    "deployment_detail": {
      "complexity": 8,
      "docker": {
        "available": false,
        "image_size": "~20GB+ (含CUDA运行时+完整ML栈)",
        "multi_stage_build": false
      },
      "kubernetes": {
        "required": false,
        "recommended": true,
        "helm_available": false
      },
      "configuration": {
        "env_vars_count": 5,
        "secrets_count": 3,
        "complexity_level": "复杂(YAML配置驱动+多代理方法+GPU环境)"
      },
      "observability": {
        "metrics_export": false,
        "structured_logging": false,
        "health_checks": false
      }
    }
  },
  "categories": {
    "tech_approach": [
      "Benchmark",
      "ICLR 2026",
      "Multi-turn"
    ],
    "use_case": [
      "Memory Evaluation",
      "Framework Comparison"
    ]
  },
  "value_propositions": [
    {
      "name": "五维记忆能力评估框架",
      "description": "通过增量多轮交互评估范式全面覆盖准确检索(AR)、测试时学习(TTL)、长程理解(LRU)、冲突解决(CR)和拒绝回答五大核心能力,结合注入一次查询多次设计哲学和GPT-4o LLM判断器,实现首个系统性评估长期记忆的多维标准化基准(ICLR 2026)。"
    },
    {
      "name": "统一代理评估平台",
      "description": "采用统一代理接口(AgentWrapper)支持15+种记忆方法的一致性评估,提供EventQA(事件时序记忆)和FactConsolidation(冲突信息整合)两个新构建数据集,通过完整的端到端评估流程(从数据加载到指标计算)和灵活的YAML配置驱动架构,简化记忆方法研究与开发。"
    }
  ],
  "huawei_cloud": {
    "overall_difficulty": "困难",
    "recommended_services": {
      "database": {
        "primary": "华为云OBS (数据集+结果) + ECS本地SQLite",
        "vector_solution": "FAISS-CPU (昇腾环境) 或 华为云CSS向量检索"
      },
      "cache": "不需要",
      "compute": {
        "primary": "华为云ECS GPU实例 (NVIDIA A100/V100 或 昇腾910B)",
        "ai_acceleration": "华为云ModelArts (API代理方法可用; 本地推理方法需NVIDIA GPU)"
      },
      "middleware": {}
    },
    "cost_estimation": {
      "small_scale": {
        "description": "单次完整评估(15种方法)",
        "monthly_cost": "¥8,000-15,000",
        "breakdown": {
          "gpu_ecs": "¥5,000 (A100 40GB * 200小时)",
          "cpu_ecs": "¥500 (32vCPU/64GB)",
          "openai_api": "¥2,000-8,000 (GPT-4o评估+长上下文代理)",
          "storage_obs": "¥200 (200GB)",
          "network": "¥300"
        }
      },
      "medium_scale": {
        "description": "持续基准测试(每周运行, 多配置)",
        "monthly_cost": "¥40,000-80,000",
        "breakdown": {
          "gpu_ecs": "¥30,000 (4xA100, 持续运行)",
          "cpu_ecs": "¥2,000",
          "llm_api": "¥5,000-30,000",
          "storage_obs": "¥500",
          "network": "¥500"
        }
      }
    },
    "special_requirements": [
      "完整NVIDIA CUDA 12运行时栈,昇腾NPU迁移困难",
      "faiss-gpu/flash_attn/bitsandbytes/deepspeed均为CUDA专用",
      "建议保留NVIDIA GPU实例用于本地模型推理方法",
      "15+种代理方法中约一半可纯API运行(不需GPU)",
      "需要大量GPU显存: NV-Embed-v2需40GB, MemoRAG需24GB+"
    ],
    "architecture_recommendations": [
      "混合部署: API代理方法使用ECS CPU实例, 本地推理方法使用GPU实例",
      "优先在华为云上运行不需要GPU的方法(长上下文API代理, BM25检索)",
      "对于需要GPU的方法,优先使用华为云NVIDIA GPU实例(如A100)",
      "使用华为云ModelArts部署Contriever/Qwen3嵌入模型替代本地GPU推理",
      "评估结果存储到华为云OBS,使用DLI(数据湖探索)进行结果分析",
      "通过华为云SWR预构建包含完整ML栈的Docker镜像,简化部署"
    ]
  }
}
{
  "name": "letta",
  "repository_url": "https://github.com/letta-ai/letta",
  "stars": 21000,
  "primary_language": "Python",
  "description": "用于构建有状态智能体的平台，具有先进记忆能力，可随时间学习和自我改进（前身为MemGPT）",
  "last_updated": "2026-01-29",
  "paper": {
    "exists": true,
    "title": "MemGPT: Towards LLMs as Operating Systems",
    "venue": "arXiv",
    "year": 2023,
    "url": "https://arxiv.org/abs/2310.08560"
  },
  "benchmarks": {},
  "tech_stack": {
    "storage": [
      "Database Persistence",
      "Memory Store"
    ],
    "frameworks": [
      "Python SDK",
      "TypeScript SDK",
      "Agent Framework"
    ],
    "languages": [
      "Python"
    ],
    "embedding_models": [
      "Model-agnostic"
    ]
  },
  "cloud_needs": {
    "storage": {
      "types": [
        "Database",
        "File Storage"
      ],
      "requirements": [
        "Persistent state management",
        "Agent history storage"
      ]
    },
    "compute": {
      "embedding": true,
      "gpu_needed": false,
      "estimated_requirements": "4-16 vCPUs for API server"
    },
    "deployment": {
      "complexity": 6,
      "containerized": true,
      "orchestration": [
        "Docker",
        "Kubernetes"
      ]
    },
    "storage_detail": {
      "vector_storage": {
        "solution": "混合方案",
        "database": "PostgreSQL+pgvector(默认) / Pinecone(可选) / TurboPuffer(可选)",
        "vector_dimension": 4096,
        "default_embedding_dim": 1024,
        "common_embedding_dim": 1536,
        "index_type": "未显式指定(依赖pgvector默认,无HNSW/IVF显式创建)",
        "evidence": "constants.py: MAX_EMBEDDING_DIM=4096, DEFAULT_EMBEDDING_DIM=1024; passage.py: Vector(MAX_EMBEDDING_DIM)使用pgvector; embedding_config.py: 默认OpenAI text-embedding-3-small维度1536; settings.py: enable_pinecone配置项; 支持TurboPuffer(use_tpuf配置); agent_manager_helper.py: np.pad填充至MAX_EMBEDDING_DIM"
      },
      "primary_database": {
        "type": "PostgreSQL(主要) / SQLite(轻量备选)",
        "min_version": "14+",
        "required_extensions": [
          "pgvector"
        ],
        "connection_pooling": {
          "pool_size": 25,
          "max_overflow": 10,
          "pool_timeout": 30,
          "pool_recycle": 1800,
          "pool_pre_ping": true,
          "pool_use_lifo": true,
          "disable_pooling_default": true
        },
        "evidence": "settings.py: pg_pool_size=25, pg_max_overflow=10, disable_sqlalchemy_pooling=True(默认使用NullPool); db.py: 使用asyncpg异步驱动+SQLAlchemy AsyncEngine; Dockerfile.simple: CREATE EXTENSION IF NOT EXISTS vector; 支持pg8000和asyncpg两种驱动; startup.sh: alembic upgrade head执行迁移"
      },
      "graph_database": {
        "required": false,
        "type": "无",
        "evidence": "代码库中未发现任何图数据库依赖(Neo4j/ArangoDB等),Letta采用三层内存架构(Core/Recall/Archival)而非知识图谱方式"
      },
      "cache": {
        "type": "Redis(可选,含NoopClient降级)",
        "min_version": "6.2+",
        "required_modules": [
          "Redis Streams"
        ],
        "evidence": "pyproject.toml: redis>=6.2.0作为可选依赖; redis_client.py: AsyncRedisClient实现连接池(max_connections=50)、分布式锁(conversation lock)、Redis Streams(xadd/xread/xrange); NoopAsyncRedisClient提供无Redis时的降级方案; startup.sh: 内置Redis或外部Redis(LETTA_REDIS_HOST); docker-compose.yml: redis:alpine; settings.py: redis_host/redis_port配置"
      },
      "data_scale": {
        "estimated_total": "中等规模,主要受Agent数量和消息历史驱动",
        "per_user_avg": "每Agent约10-100MB(含向量嵌入4096维x4字节=16KB/passage,消息历史约500字节/条)",
        "evidence": "MAX_EMBEDDING_DIM=4096,每向量16KB; EMBEDDING_BATCH_SIZE=200; 消息历史存储在PostgreSQL; archival_memory通过向量存储; CORE_MEMORY_BLOCK_CHAR_LIMIT=20000字符/block"
      },
      "performance": {
        "vector_search_latency": "<100ms(pgvector) / <50ms(Pinecone)",
        "qps_target": "依赖部署规模,单实例约100-500 QPS(FastAPI异步)",
        "p95_latency": "主要受LLM API调用延迟支配(1-30秒),向量搜索本身<100ms",
        "concurrent_connections": "pg_pool_size=25(默认NullPool无限制); Redis max_connections=50; multi_agent_concurrent_sends=50"
      }
    },
    "compute_detail": {
      "cpu": {
        "min_vcpu": 2,
        "recommended_vcpu": 4,
        "workload_type": "IO密集型",
        "evidence": "主要瓶颈在LLM API调用(网络IO)和数据库查询; FastAPI异步框架+asyncio; 无CPU密集型计算(如模型推理); event_loop_threadpool_max_workers=43"
      },
      "memory": {
        "min_gb": 4,
        "recommended_gb": 8,
        "memory_intensive_ops": [
          "Agent状态缓存(Core Memory blocks最大20000字符/block)",
          "消息历史加载(message_buffer_limit=60条)",
          "向量嵌入批处理(EMBEDDING_BATCH_SIZE=200, 每批约3.2MB@4096维)",
          "LLM请求/响应缓冲(上下文窗口最大272K tokens)"
        ],
        "oom_risk": "低",
        "evidence": "settings.py: message_buffer_limit=60, archival_memory_token_limit=8192; 无大型模型加载; 主要内存消耗来自并发Agent会话状态"
      },
      "gpu": {
        "required": false,
        "recommended": false,
        "use_case": "不需要",
        "cuda_dependency": {
          "has_direct_cuda": false,
          "custom_cuda_kernels": false,
          "gpu_libraries": [],
          "evidence": "代码库中零torch/cuda/cupy/tensorflow依赖; pyproject.toml无任何GPU相关库; 所有LLM推理通过外部API(OpenAI/Anthropic/Google等20+提供商); 所有embedding生成通过外部API(OpenAI/Pinecone/Ollama等); 搜索整个letta目录未发现任何CUDA/GPU引用"
        }
      },
      "scalability": {
        "horizontal_scaling": true,
        "stateless": true,
        "auto_scaling_metrics": [
          "CPU利用率",
          "内存利用率",
          "活跃连接数",
          "请求延迟"
        ],
        "evidence": "FastAPI无状态API服务器; 状态存储在PostgreSQL+Redis; settings.py: uvicorn_workers=1(可调整); Redis分布式锁(conversation lock)支持多实例; db_pool_monitoring监控连接池"
      },
      "serverless": {
        "suitable": false,
        "cold_start_tolerance": "不适合",
        "reasons": [
          "需要持久数据库连接(PostgreSQL连接池)",
          "Redis连接需要维持",
          "Agent会话状态需要持久化",
          "LLM API调用延迟长(1-30秒),超过典型serverless超时",
          "startup.sh包含PostgreSQL/Redis启动和数据库迁移",
          "OpenTelemetry Collector后台进程"
        ]
      },
      "concurrency": {
        "model": "异步",
        "async_framework": "asyncio + FastAPI + SQLAlchemy AsyncEngine",
        "message_queue": {
          "required": true,
          "systems": [
            "Redis Streams(内置)",
            "Temporal(可选,用于文件上传workflow)"
          ],
          "evidence": "redis_client.py: 完整Redis Streams API(xadd/xread/xrange/xrevrange/xlen/xdel/xtrim); redis_stream_manager.py: SSE流式推送; pyproject.toml: temporalio>=1.8.0; settings.py: use_lettuce_for_file_uploads(Temporal集成)"
        },
        "websocket": true,
        "streaming": true,
        "evidence": "pyproject.toml: websockets依赖; SSE流式输出(rest_api/utils.py: sse_formatter/sse_async_generator); settings.py: enable_keepalive=True, keepalive_interval=50秒; enable_cancellation_aware_streaming; aiomultiprocess>=0.9.1; async-lru>=2.0.5"
      }
    },
    "ascend_npu": {
      "compatibility_level": "不适用(无GPU需求)",
      "framework_analysis": {
        "framework": "无(纯API调用架构)",
        "framework_version": "不适用",
        "ascend_support": true,
        "cann_version": "不适用"
      },
      "migration": {
        "effort_level": "不适用",
        "blockers": [],
        "code_changes_required": [],
        "notes": "Letta本身不执行任何GPU计算,所有AI推理和嵌入生成均通过外部API完成。如需自托管LLM/Embedding模型(vLLM/Ollama),则需要考虑昇腾NPU兼容性,但这属于外部服务配置,与Letta代码库无关。"
      },
      "recommendation": "Letta是纯API调用架构,不涉及GPU/NPU计算,无需迁移。如果在华为云部署时希望使用昇腾NPU加速自托管的LLM推理服务(如vLLM+Llama),需确认vLLM对CANN/昇腾的支持情况。Letta本身通过settings.py中的vllm_api_base/ollama_base_url等配置项连接外部推理服务,与底层硬件解耦。"
    },
    "external_services": {
      "llm": {
        "providers": [
          "OpenAI(GPT-4.1/GPT-5/o3等)",
          "Anthropic(Claude 4/4.5)",
          "Google AI(Gemini 2.5/3.0)",
          "Google Vertex AI",
          "Azure OpenAI",
          "Groq",
          "Ollama(本地)",
          "vLLM(自托管)",
          "SGLang(自托管)",
          "LM Studio(本地)",
          "DeepSeek",
          "xAI(Grok)",
          "Together AI",
          "AWS Bedrock",
          "Mistral AI",
          "OpenRouter",
          "ZhipuAI(Z.AI/GLM)",
          "MiniMax"
        ],
        "embedding_models": [
          "OpenAI text-embedding-3-small(1536维,默认)",
          "OpenAI text-embedding-ada-002(1536维)",
          "Letta Free Embedding(1536维)",
          "Pinecone llama-text-embed-v2",
          "Ollama本地模型",
          "Azure OpenAI Embedding",
          "Google AI Embedding",
          "TurboPuffer(可选外部服务)"
        ],
        "local_model_support": true,
        "cost_optimization": [
          "Prompt Caching(Anthropic原生支持)",
          "多LLM提供商切换(根据成本选择最优模型)",
          "记忆压缩和汇总(减少token消耗,partial_evict_summarizer_percentage=0.30)",
          "上下文窗口智能管理(SUMMARIZATION_TRIGGER_MULTIPLIER=1.0)",
          "批量embedding处理(EMBEDDING_BATCH_SIZE=200)"
        ],
        "evidence": "settings.py: 18个LLM提供商的API Key配置; constants.py: LLM_MAX_CONTEXT_WINDOW字典包含100+模型; embedding_config.py: 支持openai/anthropic/bedrock/google_ai/google_vertex/azure/groq/ollama/vllm/pinecone等endpoint类型"
      }
    },
    "deployment_detail": {
      "complexity": 7,
      "docker": {
        "available": true,
        "image_size": "约1.5-2GB(基于ankane/pgvector,含Python/Node.js/Redis/OTel Collector)",
        "multi_stage_build": true,
        "evidence": "Dockerfile: 2阶段构建(builder+runtime); 基于ankane/pgvector:v0.5.1; 内嵌PostgreSQL+Redis+OTel Collector; 暴露端口8283(API)/5432(PG)/6379(Redis)/4317-4318(OTel); 支持ARM64和AMD64架构(TARGETARCH)"
      },
      "kubernetes": {
        "required": false,
        "recommended": true,
        "helm_available": false,
        "evidence": "cloud-needs.md提到K8s生产部署推荐; 无内置Helm chart; 支持水平扩展; Redis分布式锁支持多Pod; 但Dockerfile已包含all-in-one方案(内嵌PG+Redis)"
      },
      "configuration": {
        "env_vars_count": 80,
        "secrets_count": 15,
        "complexity_level": "复杂",
        "evidence": "settings.py: Settings类约50个配置项(LETTA_前缀), ModelSettings约25个API Key配置, ToolSettings约10个配置, TelemetrySettings约15个配置; 关键secrets: 各LLM提供商API Key(openai/anthropic/gemini等), pg_password, redis密码, pinecone_api_key, encryption_key, e2b_api_key, modal_token_secret"
      },
      "observability": {
        "metrics_export": true,
        "structured_logging": true,
        "health_checks": true,
        "evidence": "pyproject.toml: opentelemetry-api/sdk/instrumentation全套; otel/目录: tracing.py/metrics.py/resource.py/db_pool_monitoring.py/metric_registry.py; structlog>=25.4.0; log.py支持JSON格式日志(json_logging配置); health.py: /v1/health端点; Datadog集成(ddtrace>=4.2.1,datadog>=0.49.1); ClickHouse作为OTEL traces后端; Sentry集成(sentry-sdk[fastapi]); OTel Collector内嵌于Docker镜像"
      }
    }
  },
  "categories": {
    "tech_approach": [
      "Stateful Agents",
      "Continual Learning",
      "Self-Improvement"
    ],
    "use_case": [
      "Chatbots",
      "Virtual Assistants",
      "Research Agents"
    ]
  },
  "innovations": {
    "key_features": [
      "操作系统式内存管理：借鉴 OS 的虚拟内存机制，将大量对话历史存储在外部内存，动态加载到有限的上下文窗口",
      "三层内存架构：Core Memory（当前状态）、Recall Memory（最近对话）、Archival Memory（长期知识），实现不同时间粒度的记忆管理",
      "Agent 自主记忆编辑：通过 core_memory_append/replace 等工具，Agent 可主动修改自己的内存，实现真正的自学习和自我改进",
      "完全模型无关：支持 OpenAI、Anthropic、Google、Groq、Ollama 等 20+ LLM 提供商，统一的 API 接口",
      "企业级 Agent 平台：提供完整的 REST API、Python/TypeScript SDK、用户管理、组织隔离、审计日志",
      "无缝会话连续性：Agent 在多个会话中保持一致的状态和记忆，支持长期关系建立",
      "工具生态系统：支持自定义工具、MCP（Model Context Protocol）集成、Composio 平台工具",
      "流式响应和实时交互：支持 SSE 流式输出，实时工具调用和结果展示",
      "多模态 Agent 支持：结合 Letta Code CLI，支持代码执行、文件操作、浏览器自动化等高级能力",
      "本地和云端灵活部署：支持 Docker Compose 本地开发、Kubernetes 生产部署、Letta Cloud 托管服务"
    ],
    "improvements": [
      "突破上下文窗口限制：通过虚拟上下文管理，Agent 可以处理远超 LLM 上下文窗口的对话长度（理论上无限）",
      "降低 Token 成本：只将相关记忆加载到上下文中，通过智能汇总和压缩，减少 50-70% 的 token 使用量",
      "提升 Agent 记忆准确性：三层内存架构确保关键信息不会丢失，向量检索确保相关历史能被精准召回",
      "增强 Agent 自主性：Agent 可以主动决定何时更新记忆、删除过时信息、汇总长期经验，而非被动接受人类指令",
      "改进多租户隔离：Organization 级别的数据隔离，确保不同用户/团队的 Agent 数据完全独立",
      "优化开发者体验：直观的 SDK 设计、丰富的示例代码、活跃的社区支持，降低 Agent 开发门槛",
      "提高系统可维护性：标准化的 Agent 配置管理、版本控制、A/B 测试能力，便于迭代和优化",
      "扩展性和性能：支持水平扩展、异步任务处理、连接池管理，可支撑大规模并发 Agent",
      "安全和合规：API Key 认证、RBAC 权限控制、审计日志、数据加密，满足企业安全要求",
      "降低运维复杂度：托管向量数据库集成、自动备份恢复、健康检查和监控，简化生产部署"
    ],
    "user_value": [
      "构建真正有记忆的 AI 助手：用户可以与 Agent 建立长期关系，Agent 记得所有历史对话和偏好",
      "降低开发成本和时间：开箱即用的 Agent 框架，无需从零实现内存管理和状态持久化，加速产品上线",
      "提升用户体验：Agent 能够提供个性化、上下文相关的回复，避免重复提问，增强用户满意度",
      "支持复杂业务场景：长期客户关系管理、个性化推荐、知识库问答、代码助手等需要记忆的应用",
      "灵活的技术选型：不锁定特定 LLM 提供商或向量数据库，可根据成本、性能、合规要求自由选择",
      "透明的成本控制：通过记忆压缩和智能加载，显著降低 LLM API 调用成本，适合大规模部署",
      "企业级可靠性：高可用架构、数据备份、故障恢复、监控告警，保障业务连续性",
      "社区和生态支持：活跃的开源社区、丰富的插件和工具、定期更新和 bug 修复",
      "隐私和数据主权：支持本地部署和私有化方案，数据完全掌控在用户手中",
      "未来可扩展性：随着业务增长，可无缝升级到更强的模型、更大的存储、更多的并发，无需重构架构"
    ]
  },
  "use_cases": {
    "scenarios": [
      "长期对话型 AI 助手：个人助理、虚拟伴侣、心理咨询机器人，需要记住用户偏好和历史对话",
      "企业级智能客服：记录客户历史问题、偏好、投诉记录，提供个性化和连续的服务体验",
      "个性化推荐系统：根据用户长期行为和反馈，持续学习和优化推荐结果（内容、产品、服务）",
      "AI 代码助手集成：如 Letta Code CLI，记住项目上下文、代码风格、用户习惯，提供更精准的代码建议",
      "教育和培训 Agent：追踪学生学习进度、知识掌握情况、学习习惯，提供个性化辅导",
      "游戏 NPC 记忆系统：游戏角色记住玩家的选择、对话、关系，创造沉浸式的故事体验",
      "医疗健康助手：记录患者病史、用药记录、生活习惯，辅助医生诊断和健康管理（需符合 HIPAA）",
      "销售和 CRM Agent：记录客户互动历史、购买偏好、谈判要点，辅助销售人员跟进",
      "研究和知识管理：持续积累领域知识、实验结果、文献笔记，辅助研究人员探索",
      "多轮复杂任务执行：如旅行规划、项目管理、事件组织，需要跨多个会话持续推进和记忆上下文"
    ],
    "companies": [
      "Letta AI（开发团队自身）：提供 Letta Cloud 托管服务和企业支持",
      "开源社区贡献者：来自全球的 100+ 开发者为 Letta 项目贡献代码和插件"
    ]
  },
  "value_propositions": [
    {
      "name": "操作系统式内存管理",
      "description": "借鉴OS虚拟内存机制实现三层内存架构(Core Memory当前状态+Recall Memory最近对话+Archival Memory长期知识),通过Agent自主记忆编辑工具突破上下文窗口限制,理论上支持无限对话长度的同时降低50-70% token使用量。"
    },
    {
      "name": "企业级Agent平台",
      "description": "提供完整的REST API、Python/TypeScript SDK、用户管理和组织隔离,支持OpenAI、Anthropic、Google等20+ LLM提供商,通过流式响应、工具生态系统和MCP集成,实现生产就绪的无缝会话连续性和多租户安全隔离。"
    }
  ],
  "huawei_cloud": {
    "overall_difficulty": "中等",
    "recommended_services": {
      "database": {
        "primary": "RDS PostgreSQL 14+(需开启pgvector扩展)",
        "vector_solution": "RDS PostgreSQL + pgvector(推荐) 或 外接Pinecone(保持SaaS)",
        "notes": "华为云RDS PostgreSQL需确认pgvector扩展支持;如不支持,可使用GaussDB或自建PostgreSQL+pgvector容器;asyncpg驱动兼容标准PostgreSQL协议"
      },
      "cache": {
        "service": "DCS Redis 6.x+",
        "notes": "Letta使用标准Redis协议+Redis Streams,DCS Redis完全兼容;Redis为可选组件,有NoopClient降级;需支持Redis Streams命令(xadd/xread等)"
      },
      "compute": {
        "primary": "ECS通用型s6/c6(2-8 vCPU, 4-16GB RAM)",
        "container": "CCE(云容器引擎)用于K8s部署",
        "ai_acceleration": "不需要(Letta本身无GPU计算需求)",
        "notes": "如需自托管LLM推理服务,可额外部署ModelArts+昇腾NPU运行vLLM/Ollama,Letta通过API连接"
      },
      "middleware": {
        "message_queue": "不需要额外MQ(Redis Streams已满足)",
        "api_gateway": "APIG(API网关)用于流量管理和鉴权",
        "load_balancer": "ELB弹性负载均衡",
        "dns": "云解析DNS",
        "cdn": "CDN加速(如有前端UI)"
      },
      "observability": {
        "monitoring": "AOM(应用运维管理)或对接OTel Collector到LTS",
        "logging": "LTS(云日志服务)",
        "tracing": "APM(应用性能管理)接收OTel数据",
        "notes": "Letta内嵌OTel Collector,可配置OTLP endpoint指向华为云AOM/APM; 也可配置ClickHouse后端"
      }
    },
    "cost_estimation": {
      "small_scale": {
        "description": "100用户/100 Agent,每Agent月500条消息",
        "monthly_cost": "¥800-1,500",
        "breakdown": {
          "compute_ecs": "¥300-500(ECS s6.large.2, 2vCPU/4GB x1)",
          "database_rds": "¥400-600(RDS PostgreSQL主备版, 2核4GB, 100GB SSD)",
          "cache_redis": "¥0-100(DCS Redis可选, 基础版2GB)",
          "llm_api": "¥200-300(外部LLM API成本,非华为云费用)",
          "network": "¥50-100(EIP+带宽)"
        }
      },
      "medium_scale": {
        "description": "1000用户/1000 Agent,每Agent月1000条消息",
        "monthly_cost": "¥3,000-6,000",
        "breakdown": {
          "compute_ecs_or_cce": "¥800-1,500(ECS c6.xlarge.2 x2-3台或CCE集群)",
          "database_rds": "¥1,000-2,000(RDS PostgreSQL高可用版, 4核8GB, 500GB SSD + 只读副本)",
          "cache_redis": "¥200-400(DCS Redis主备版, 4GB)",
          "llm_api": "¥3,000-5,000(外部LLM API成本,非华为云费用)",
          "network_elb": "¥200-400(ELB+EIP+带宽)",
          "observability": "¥100-200(AOM+LTS基础版)"
        }
      }
    },
    "special_requirements": [
      "pgvector扩展支持:华为云RDS PostgreSQL需确认是否支持pgvector扩展,若不支持需使用自建容器化PostgreSQL+pgvector(ankane/pgvector镜像)",
      "Redis Streams支持:确认DCS Redis版本支持Streams命令(Redis 5.0+)",
      "OTel Collector集成:Letta Docker镜像内嵌OTel Collector,可配置OTLP endpoint指向华为云APM",
      "多LLM API连通性:需确保ECS/CCE可访问外部LLM API(OpenAI/Anthropic等),可能需要配置NAT网关或代理",
      "容器镜像兼容:Letta Dockerfile基于ankane/pgvector(Debian),支持ARM64,可运行在鲲鹏架构上",
      "数据库迁移:Letta使用Alembic进行数据库迁移,需确保RDS PostgreSQL权限允许CREATE EXTENSION"
    ],
    "architecture_recommendations": [
      "推荐架构:ECS/CCE + RDS PostgreSQL(pgvector) + DCS Redis + ELB + APIG",
      "开发环境:单台ECS运行Letta Docker镜像(all-in-one,内嵌PG+Redis),成本最低",
      "生产环境:将PostgreSQL和Redis外置为华为云托管服务,Letta API服务部署在CCE中实现水平扩展",
      "LLM服务:初期使用外部API(通过NAT网关访问),大规模后可在ModelArts部署自托管模型",
      "监控体系:利用Letta内嵌的OTel Collector,配置OTLP endpoint到华为云AOM,实现全链路观测",
      "安全合规:使用APIG进行API鉴权和流量控制;使用DEW(数据加密服务)管理API Key等密钥;配置VPC安全组限制数据库和Redis访问",
      "灾备方案:RDS PostgreSQL多可用区部署;DCS Redis主备版;ECS/CCE跨可用区调度"
    ]
  }
}
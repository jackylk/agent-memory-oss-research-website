# LongMemEval åŽä¸ºäº‘é€‚é…æ€§åˆ†æž

> åŸºäºŽ LongMemEval (ICLR 2025) ä»£ç åº“åˆ†æž,è¯„ä¼°åœ¨åŽä¸ºäº‘ä¸Šçš„éƒ¨ç½²å¯è¡Œæ€§

## 1. é€‚é…æ€§æ€»è§ˆ

### æ•´ä½“è¯„ä¼°

| ç»´åº¦ | è¯„çº§ | è¯´æ˜Ž |
|------|------|------|
| **é€‚é…éš¾åº¦** | ðŸ”´ å›°éš¾ | é‡åº¦ä¾èµ–CUDAç”Ÿæ€(vLLM+FlashAttention),æ˜‡è…¾NPUéœ€å¤§é‡é€‚é… |
| **æ ¸å¿ƒæŒ‘æˆ˜** | GPUæŽ¨ç†æ¡†æž¶ | vLLMå’ŒFlashAttentionä¸åŽŸç”Ÿæ”¯æŒæ˜‡è…¾NPU |
| **æŽ¨èåº¦** | â­â­â­â˜†â˜† | é€‚åˆéƒ¨ç½²ä½†éœ€æ··åˆæ–¹æ¡ˆ,NVIDIA GPUå®žä¾‹ä¼˜å…ˆ |

### å…³é”®å‘çŽ°

**âœ… åŽä¸ºäº‘å®Œå…¨æ”¯æŒçš„æ ¸å¿ƒèƒ½åŠ›**:
- å¯¹è±¡å­˜å‚¨(OBS,æ•°æ®é›†+æ¨¡åž‹æƒé‡+ç»“æžœ)
- ECSè®¡ç®—å®žä¾‹(CPUç”¨äºŽBM25æ£€ç´¢,GPUç”¨äºŽæ¨¡åž‹æŽ¨ç†)
- ModelArtsæŽ¨ç†æœåŠ¡(å¯æ›¿ä»£éƒ¨åˆ†vLLMåŠŸèƒ½)
- ç›‘æŽ§å‘Šè­¦(CES + APM)
- å®¹å™¨ç¼–æŽ’(CCE)

**âš ï¸ éœ€è¦é€‚é…æˆ–æ›¿ä»£çš„æœåŠ¡**:
- **vLLMæŽ¨ç†æ¡†æž¶**:ä¸åŽŸç”Ÿæ”¯æŒæ˜‡è…¾NPU,éœ€ä½¿ç”¨vLLM-ascendåˆ†æ”¯æˆ–åŽä¸ºMindIE
- **FlashAttention**:CUDAä¸“ç”¨,éœ€æ›¿æ¢ä¸ºæ˜‡è…¾CANN FlashAttention
- **å¤šGPUå¹¶è¡Œ**:Tensor Paralleléœ€è¦é€‚é…æ˜‡è…¾HCCLé€šä¿¡åº“

**ðŸ’¡ æˆæœ¬ä¼˜åŠ¿**:
- ä½¿ç”¨åŽä¸ºç›˜å¤å¤§æ¨¡åž‹æ›¿ä»£OpenAI â†’ è¯„ä¼°æˆæœ¬é™ä½Ž50-60%
- å°è§„æ¨¡éƒ¨ç½²æœˆæˆæœ¬:Â¥5,000-8,000(vs AWS ~Â¥12,000)
- BM25æ£€ç´¢çº¯CPUè¿è¡Œ,æ— éœ€GPUæˆæœ¬

---

## 2. åŽä¸ºäº‘ä¼˜åŠ¿ä¸ŽæœåŠ¡æ˜ å°„

### 2.1 å¯¹è±¡å­˜å‚¨ âœ… å®Œå…¨æ”¯æŒ

**LongMemEvaléœ€æ±‚**:
- æ•°æ®é›†å­˜å‚¨(LongMemEval_S: 115k tokens ~5MB, LongMemEval_M: ~50MB)
- æ¨¡åž‹æƒé‡ç¼“å­˜(100-500GB: Llama 70B ~140GB, GTE-7B ~14GB)
- å®žéªŒæ—¥å¿—(5-20GB)
- è¯„ä¼°ç»“æžœ(1-5GB)

**åŽä¸ºäº‘è§£å†³æ–¹æ¡ˆ**:
```yaml
æœåŠ¡: OBS (å¯¹è±¡å­˜å‚¨æœåŠ¡)
å­˜å‚¨ç±»åž‹:
  - æ ‡å‡†å­˜å‚¨: æ¨¡åž‹æƒé‡ã€æ´»è·ƒæ•°æ®é›†(Â¥0.099/GB/æœˆ)
  - ä½Žé¢‘è®¿é—®: åŽ†å²å®žéªŒæ—¥å¿—(Â¥0.06/GB/æœˆ)
  - å½’æ¡£å­˜å‚¨: é•¿æœŸå¤‡ä»½(Â¥0.033/GB/æœˆ)
ç”Ÿå‘½å‘¨æœŸç­–ç•¥:
  - 30å¤©åŽ: æ ‡å‡† â†’ ä½Žé¢‘(èŠ‚çœ40%)
  - 90å¤©åŽ: ä½Žé¢‘ â†’ å½’æ¡£(èŠ‚çœ80%)
æœˆæˆæœ¬: Â¥200-500(500GBæ¨¡åž‹+æ•°æ®)
```

**ä¼˜åŠ¿**:
- âœ… S3å…¼å®¹API,boto3æ— éœ€ä¿®æ”¹
- âœ… 11ä¸ª9çš„æ•°æ®æŒä¹…æ€§
- âœ… æ¯”AWS S3ä¾¿å®œ30%

---

### 2.2 è®¡ç®—èµ„æº âš ï¸ éœ€è¦é€‚é…

**LongMemEvaléœ€æ±‚**:
- BM25æ£€ç´¢:16-32 vCPUs, 32-64GB RAM(æ— GPU)
- Denseæ£€ç´¢(GTE-Qwen2-7B):1-4x A100 40GB
- LLMæŽ¨ç†(Llama 70B):4x A100 40GB(Tensor Parallel)
- è¯„ä¼°é˜¶æ®µ:CPUå³å¯(GPT-4o APIè°ƒç”¨)

**åŽä¸ºäº‘è§£å†³æ–¹æ¡ˆ**:

#### æ–¹æ¡ˆ1:NVIDIA GPUå®žä¾‹ â­ å¼ºçƒˆæŽ¨è
```yaml
æœåŠ¡: ECS GPUå®žä¾‹
è§„æ ¼:
  - å°è§„æ¨¡: pi2.8xlarge.4 (8æ ¸32GB + 1x V100 32GB)
  - ä¸­è§„æ¨¡: pi2.16xlarge.4 (16æ ¸64GB + 2x V100 32GB)
  - å¤§è§„æ¨¡: pi2.32xlarge.8 (32æ ¸128GB + 4x V100 32GB)
ç”¨é€”: vLLMæŽ¨ç†ã€Denseæ£€ç´¢
æœˆæˆæœ¬: Â¥3,000-12,000(æŒ‰éœ€100-400å°æ—¶)
```

**ä¼˜åŠ¿**:
- âœ… å®Œå…¨å…¼å®¹CUDAç”Ÿæ€
- âœ… é›¶ä»£ç ä¿®æ”¹
- âœ… vLLM/FlashAttentionå¼€ç®±å³ç”¨

#### æ–¹æ¡ˆ2:æ˜‡è…¾NPUå®žä¾‹(éœ€è¦é€‚é…)
```yaml
æœåŠ¡: ECSæ˜‡è…¾å®žä¾‹
è§„æ ¼: ai1s.8xlarge.8 (8æ ¸32GB + æ˜‡è…¾910B 32GB)
æŒ‘æˆ˜:
  - vLLMéœ€æ›¿æ¢ä¸ºvLLM-ascendæˆ–MindIE
  - FlashAttentionéœ€æ›¿æ¢ä¸ºCANNåŽŸç”Ÿå®žçŽ°
  - Tensor Paralleléœ€é€‚é…HCCLé€šä¿¡åº“
é€‚é…å·¥ä½œé‡: 5-10äººå¤©
æœˆæˆæœ¬: Â¥2,000-8,000(æ¯”NVIDIAä¾¿å®œ30%)
```

#### æ–¹æ¡ˆ3:æ··åˆéƒ¨ç½² â­ æœ€ä½³å®žè·µ
```yaml
CPUä»»åŠ¡(BM25æ£€ç´¢/è¯„ä¼°):
  - ECSé€šç”¨å®žä¾‹ c7.4xlarge.2 (16æ ¸32GB)
  - æœˆæˆæœ¬: Â¥500

GPUä»»åŠ¡(Denseæ£€ç´¢/LLMæŽ¨ç†):
  - NVIDIA GPUå®žä¾‹ pi2.16xlarge.4
  - æœˆæˆæœ¬: Â¥6,000(200å°æ—¶)

æ€»æˆæœ¬: Â¥6,500/æœˆ
```

---

### 2.3 AIæŽ¨ç†æœåŠ¡ âš ï¸ å¯ç”¨ä½†éœ€é€‚é…

**LongMemEvaléœ€æ±‚**:
- LLMè¯„ä¼°(GPT-4o API)
- LLMç”Ÿæˆ(Llama 70Bè‡ªæ‰˜ç®¡)
- Denseæ£€ç´¢(GTE-Qwen2-7B)

**åŽä¸ºäº‘è§£å†³æ–¹æ¡ˆ**:

#### ModelArtsåœ¨çº¿æŽ¨ç†(æ›¿ä»£vLLM)
```yaml
æœåŠ¡: ModelArtsæŽ¨ç†æœåŠ¡
æ¨¡åž‹:
  - GTE-Qwen2-7B(é˜¿é‡Œ,åŽä¸ºä¼˜åŒ–ç‰ˆ)
  - Llama 3.1 8B/70B
è§„æ ¼: modelarts.bm.xlarge(1xæ˜‡è…¾910B)
ä¼˜ç‚¹:
  - æ‰˜ç®¡æœåŠ¡,é›¶è¿ç»´
  - è‡ªåŠ¨æ‰©ç¼©å®¹
  - æ˜‡è…¾åŽŸç”Ÿä¼˜åŒ–
ç¼ºç‚¹:
  - æ€§èƒ½å¯èƒ½ç•¥ä½ŽäºŽvLLM
  - APIéœ€è¦é€‚é…
æœˆæˆæœ¬: Â¥3,000-8,000(æŒ‰è°ƒç”¨é‡)
```

#### åŽä¸ºç›˜å¤å¤§æ¨¡åž‹(æ›¿ä»£OpenAI)
```yaml
æœåŠ¡: ç›˜å¤å¤§æ¨¡åž‹API
æ¨¡åž‹: ç›˜å¤NLP-70B(æ›¿ä»£GPT-4)
ç”¨é€”: LLM-as-Judgeè¯„ä¼°
æˆæœ¬: æ¯”OpenAIä¾¿å®œ50-60%
æœˆæˆæœ¬: Â¥500-2,000(vs OpenAI Â¥1,000-4,000)
```

**ä»£ç é€‚é…ç¤ºä¾‹**:
```python
# åŽŸOpenAIè°ƒç”¨
from openai import OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "è¯„ä¼°ç­”æ¡ˆ"}]
)

# æ›¿æ¢ä¸ºåŽä¸ºç›˜å¤
import requests
pangu_url = "https://pangu.cn-north-4.myhuaweicloud.com/v1/infers/xxx"
response = requests.post(pangu_url, headers={"X-Auth-Token": os.getenv("PANGU_TOKEN")},
    json={"inputs": "è¯„ä¼°ç­”æ¡ˆ", "parameters": {"max_new_tokens": 512}})
```

---

### 2.4 å®¹å™¨ç¼–æŽ’ âœ… å®Œå…¨æ”¯æŒ

**åŽä¸ºäº‘è§£å†³æ–¹æ¡ˆ**:
```yaml
æœåŠ¡: CCE (äº‘å®¹å™¨å¼•æ“Ž)
é›†ç¾¤: æ ‡å‡†ç‰ˆCCE
èŠ‚ç‚¹:
  - CPUèŠ‚ç‚¹: s7.2xlarge.2 (8æ ¸16GB) Ã— 3
  - GPUèŠ‚ç‚¹: pi2.16xlarge.4 (16æ ¸64GB+2xV100) Ã— 2
è‡ªåŠ¨æ‰©ç¼©å®¹: HPA + CA
æœˆæˆæœ¬: Â¥8,000-15,000
```

---

### 2.5 ç›‘æŽ§å‘Šè­¦ âœ… å®Œå…¨æ”¯æŒ

**åŽä¸ºäº‘è§£å†³æ–¹æ¡ˆ**:
```yaml
æœåŠ¡: CES + APM + LTS
ç›‘æŽ§æŒ‡æ ‡:
  - GPUåˆ©ç”¨çŽ‡ã€æ˜¾å­˜ä½¿ç”¨
  - APIè°ƒç”¨é‡ã€æˆåŠŸçŽ‡
  - è¯„ä¼°å‡†ç¡®çŽ‡ã€å»¶è¿Ÿ
å‘Šè­¦è§„åˆ™:
  - GPUæ˜¾å­˜>90%(æ‰©å®¹)
  - APIé”™è¯¯çŽ‡>5%(å‘Šè­¦)
  - è¯„ä¼°å‡†ç¡®çŽ‡<60%(å‘Šè­¦)
æœˆæˆæœ¬: Â¥100-300
```

---

## 3. åŽä¸ºäº‘å·®è·ä¸ŽæŒ‘æˆ˜

### 3.1 âŒ vLLMæŽ¨ç†æ¡†æž¶ - éœ€è¦æ›¿ä»£

**LongMemEvalä¾èµ–**:
- vLLM 0.5.3:é«˜æ€§èƒ½LLMæŽ¨ç†æœåŠ¡å™¨
- FlashAttention 2.6.3:CUDAè‡ªå®šä¹‰å†…æ ¸
- Tensor Parallel:4å¡å¹¶è¡ŒæŽ¨ç†Llama 70B

**åŽä¸ºäº‘çŽ°çŠ¶**:
- âŒ vLLMä¸åŽŸç”Ÿæ”¯æŒæ˜‡è…¾NPU
- âŒ FlashAttentionæ˜¯CUDAä¸“ç”¨åº“
- âŒ éœ€è¦é€‚é…æ˜‡è…¾HCCLå¤šå¡é€šä¿¡

**æ›¿ä»£æ–¹æ¡ˆ**:

#### æ–¹æ¡ˆ1:ä½¿ç”¨NVIDIA GPUå®žä¾‹ â­ æŽ¨è
```yaml
é…ç½®: pi2.32xlarge.8 (4x V100 32GB)
ä¼˜ç‚¹:
  - é›¶ä»£ç ä¿®æ”¹
  - vLLM/FlashAttentionå¼€ç®±å³ç”¨
  - æ€§èƒ½æœ€ä¼˜
ç¼ºç‚¹:
  - æˆæœ¬è¾ƒé«˜
æœˆæˆæœ¬: Â¥10,000-15,000(400å°æ—¶)
```

#### æ–¹æ¡ˆ2:åŽä¸ºMindIEæŽ¨ç†å¼•æ“Ž
```yaml
æœåŠ¡: ModelArts MindIE
æ¨¡åž‹: Llama 70B(æ˜‡è…¾ä¼˜åŒ–ç‰ˆ)
ä¼˜ç‚¹:
  - æ˜‡è…¾åŽŸç”Ÿä¼˜åŒ–
  - æ‰˜ç®¡æœåŠ¡
  - æˆæœ¬é™ä½Ž30%
ç¼ºç‚¹:
  - APIéœ€è¦é€‚é…
  - æ€§èƒ½å¯èƒ½ç•¥ä½ŽäºŽvLLM
é€‚é…å·¥ä½œé‡: 3-5äººå¤©
æœˆæˆæœ¬: Â¥7,000-10,000
```

#### æ–¹æ¡ˆ3:ä½¿ç”¨å¤–éƒ¨API(ç®€åŒ–æ–¹æ¡ˆ)
```yaml
LLMæŽ¨ç†: åŽä¸ºç›˜å¤å¤§æ¨¡åž‹API
æ£€ç´¢: ModelArtséƒ¨ç½²GTE-Qwen2-7B
ä¼˜ç‚¹:
  - é›¶GPUæˆæœ¬
  - é›¶è¿ç»´
  - å¿«é€Ÿä¸Šçº¿
ç¼ºç‚¹:
  - APIè°ƒç”¨æˆæœ¬è¾ƒé«˜
  - æ€§èƒ½å—ç½‘ç»œå»¶è¿Ÿå½±å“
æœˆæˆæœ¬: Â¥3,000-8,000
```

---

### 3.2 âš ï¸ æ˜‡è…¾NPUé€‚é…å·¥ä½œé‡

**ä»£ç ä¿®æ”¹æ¸…å•**:
1. æ›¿æ¢vLLMä¸ºvLLM-ascendæˆ–MindIE(3-5äººå¤©)
2. æ›¿æ¢FlashAttentionä¸ºCANNåŽŸç”Ÿå®žçŽ°(2-3äººå¤©)
3. ä¿®æ”¹torch.cudaè°ƒç”¨ä¸ºtorch_npu(1äººå¤©)
4. è°ƒæ•´Tensor Parallelé…ç½®ä½¿ç”¨HCCL(1-2äººå¤©)
5. æµ‹è¯•å’Œè°ƒä¼˜(2-3äººå¤©)

**æ€»è®¡**: 9-16äººå¤©å·¥ä½œé‡

**æŽ¨è**:
- å¿«é€Ÿä¸Šçº¿:ä½¿ç”¨NVIDIA GPUå®žä¾‹
- é•¿æœŸä¼˜åŒ–:é€æ­¥è¿ç§»åˆ°æ˜‡è…¾NPU

---

## 4. éƒ¨ç½²æž¶æž„æ–¹æ¡ˆ

### 4.1 å°è§„æ¨¡æž¶æž„(ç ”ç©¶å®žéªŒ,LongMemEval_Så•æ¬¡è¿è¡Œ)

```
åŽä¸ºäº‘éƒ¨ç½²æž¶æž„:

CPUèŠ‚ç‚¹(BM25æ£€ç´¢+è¯„ä¼°):
â”œâ”€â”€ ECS c7.4xlarge.2 (16æ ¸32GB)
â”‚   â”œâ”€â”€ BM25ç¨€ç–æ£€ç´¢(10è¿›ç¨‹å¹¶è¡Œ)
â”‚   â””â”€â”€ è¯„ä¼°æŒ‡æ ‡è®¡ç®—

GPUèŠ‚ç‚¹(Denseæ£€ç´¢+LLMæŽ¨ç†):
â”œâ”€â”€ ECS pi2.16xlarge.4 (16æ ¸64GB + 2x V100)
â”‚   â”œâ”€â”€ GTE-Qwen2-7Bæ£€ç´¢(2å°æ—¶)
â”‚   â””â”€â”€ Llama 70Bç”Ÿæˆ(éœ€4x V100,å‡çº§å®žä¾‹)

å¤–éƒ¨API:
â”œâ”€â”€ OpenAI GPT-4o(è¯„ä¼°)
â””â”€â”€ åŽä¸ºç›˜å¤å¤§æ¨¡åž‹(å¤‡é€‰)

å­˜å‚¨å±‚:
â”œâ”€â”€ OBSæ ‡å‡†å­˜å‚¨
â”‚   â”œâ”€â”€ æ•°æ®é›†: longmemeval_s.json (5MB)
â”‚   â”œâ”€â”€ æ¨¡åž‹æƒé‡: 500GB (Llama 70B, GTEç­‰)
â”‚   â””â”€â”€ ç»“æžœ: 5-10MB/æ¬¡

ç›‘æŽ§:
â””â”€â”€ CES + APM + LTS
```

**æœˆæˆæœ¬ä¼°ç®—**(å•æ¬¡å®žéªŒ):
| æœåŠ¡ | è§„æ ¼ | æˆæœ¬ |
|------|------|------|
| CPU ECS | c7.4xlarge.2 Ã— 8å°æ—¶ | Â¥50 |
| GPU ECS | pi2.32xlarge.8 Ã— 10å°æ—¶ | Â¥300 |
| OBSå­˜å‚¨ | 500GBæ ‡å‡†å­˜å‚¨ | Â¥50 |
| OpenAI API | GPT-4oè¯„ä¼°500æ¬¡ | Â¥20 |
| NATç½‘å…³+å¸¦å®½ | 10Mbps | Â¥3 |
| **å•æ¬¡æ€»è®¡** | | **Â¥423** |

**æœˆåº¦æˆæœ¬**(10æ¬¡å®žéªŒ): Â¥5,000-8,000

**vs AWSæˆæœ¬**: AWSçº¦Â¥12,000/æœˆ,åŽä¸ºäº‘èŠ‚çœ**33-58%**

---

### 4.2 ä¸­è§„æ¨¡æž¶æž„(æŒç»­è¯„ä¼°,æ¯å‘¨è¿è¡Œ)

**æœˆæˆæœ¬ä¼°ç®—**: Â¥30,000-60,000
| æœåŠ¡ | è§„æ ¼ | æœˆæˆæœ¬ |
|------|------|--------|
| CCEé›†ç¾¤ | 3x s7.2xlarge.2 (CPUèŠ‚ç‚¹) | Â¥2,000 |
| GPU ECS | 4xæ˜‡è…¾910BæŒç»­è¿è¡Œ | Â¥20,000 |
| ModelArtsæŽ¨ç† | GTE-Qwen2-7Bæ‰˜ç®¡ | Â¥5,000 |
| OBS | 2TBæ··åˆå­˜å‚¨ | Â¥500 |
| ç›˜å¤å¤§æ¨¡åž‹ | æ›¿ä»£OpenAI API | Â¥2,000-8,000 |
| ç½‘ç»œ+ç›‘æŽ§ | NAT+CES+APM | Â¥500 |
| **æ€»è®¡** | | **Â¥30,000-36,000** |

**vs AWSæˆæœ¬**: AWSçº¦Â¥80,000/æœˆ,åŽä¸ºäº‘èŠ‚çœ**55-62%**

---

### 4.3 å¤§è§„æ¨¡æž¶æž„(ç”Ÿäº§çŽ¯å¢ƒ,å¤šæ¨¡åž‹å¯¹æ¯”)

**æœˆæˆæœ¬ä¼°ç®—**: Â¥80,000-150,000
| æœåŠ¡ | è§„æ ¼ | æœˆæˆæœ¬ |
|------|------|--------|
| CCEä¼ä¸šç‰ˆ | 10èŠ‚ç‚¹é›†ç¾¤ | Â¥5,000 |
| GPU ECS | 8x NVIDIA V100æŒç»­è¿è¡Œ | Â¥50,000 |
| ModelArts | å¤šæ¨¡åž‹æ‰˜ç®¡ | Â¥15,000 |
| OBS | 5TBæ··åˆå­˜å‚¨ | Â¥1,000 |
| RDS PostgreSQL | 8æ ¸32GB(å®žéªŒç»“æžœ) | Â¥2,000 |
| ç›˜å¤å¤§æ¨¡åž‹ | å¤§é‡è°ƒç”¨ | Â¥5,000-20,000 |
| ç½‘ç»œ+CDN | 100Mbps+æµé‡ | Â¥2,000 |
| APM+ç›‘æŽ§ | ä¼ä¸šçº§ | Â¥1,000 |
| **æ€»è®¡** | | **Â¥81,000-96,000** |

**vs AWSæˆæœ¬**: AWSçº¦Â¥180,000/æœˆ,åŽä¸ºäº‘èŠ‚çœ**46-55%**

---

## 5. è¿ç§»å»ºè®®

### 5.1 å¿«é€Ÿä¸Šçº¿è·¯å¾„(2-4å‘¨)

**ç¬¬1å‘¨:åŸºç¡€è®¾æ–½å‡†å¤‡**
```
Day 1-2: åˆ›å»ºVPCã€å®‰å…¨ç»„ã€NATç½‘å…³
Day 3-4: åˆ›å»ºOBS Bucket,ä¸Šä¼ æ•°æ®é›†å’Œæ¨¡åž‹æƒé‡
Day 5: åˆ›å»ºNVIDIA GPU ECSå®žä¾‹(æŽ¨è)
Day 6-7: å®‰è£…CUDAã€PyTorchã€vLLMã€FlashAttention
```

**ç¬¬2å‘¨:åº”ç”¨éƒ¨ç½²**
```
Day 8-9: é…ç½®BM25æ£€ç´¢(CPUå®žä¾‹)
Day 10-11: éƒ¨ç½²vLLMæœåŠ¡(GPUå®žä¾‹)
Day 12: é…ç½®Denseæ£€ç´¢(GTE-Qwen2-7B)
Day 13-14: é›†æˆOpenAI/ç›˜å¤APIè¿›è¡Œè¯„ä¼°
```

**ç¬¬3å‘¨:æµ‹è¯•å’Œä¼˜åŒ–**
```
Day 15-16: è¿è¡ŒLongMemEval_Så•æ¬¡å®Œæ•´è¯„ä¼°
Day 17-18: æ€§èƒ½è°ƒä¼˜(æ‰¹å¤„ç†ã€å¹¶è¡Œåº¦)
Day 19-20: é…ç½®CES/APMç›‘æŽ§
Day 21: æˆæœ¬ä¼˜åŒ–(Spotå®žä¾‹ã€ç”Ÿå‘½å‘¨æœŸç­–ç•¥)
```

**ç¬¬4å‘¨:ä¸Šçº¿å’Œè¿­ä»£**
```
Day 22-24: ç”Ÿäº§çŽ¯å¢ƒéƒ¨ç½²
Day 25-28: æŒç»­ç›‘æŽ§å’Œè°ƒä¼˜
```

---

### 5.2 æˆæœ¬ä¼˜åŒ–ç­–ç•¥

**ðŸ’° é™ä½Ž50-60% APIæˆæœ¬**:
- ä½¿ç”¨åŽä¸ºç›˜å¤å¤§æ¨¡åž‹æ›¿ä»£OpenAI GPT-4o
- è¯„ä¼°æˆæœ¬:Â¥1,000-4,000 â†’ Â¥500-2,000
- å¹´èŠ‚çœ:Â¥6,000-24,000

**ðŸ’° é™ä½Ž30% GPUæˆæœ¬**:
- ä½¿ç”¨æ˜‡è…¾910Bæ›¿ä»£NVIDIA V100(éœ€é€‚é…)
- æˆ–ä½¿ç”¨ModelArtsæ‰˜ç®¡æœåŠ¡
- æœˆèŠ‚çœ:Â¥3,000-5,000

**ðŸ’° é™ä½Ž70% å­˜å‚¨æˆæœ¬**:
- OBSç”Ÿå‘½å‘¨æœŸè‡ªåŠ¨å½’æ¡£
- 30å¤©åŽè½¬ä½Žé¢‘,90å¤©åŽè½¬å½’æ¡£
- æœˆèŠ‚çœ:Â¥350-700(500GB-2TB)

**æ€»æˆæœ¬ä¼˜åŒ–æ•ˆæžœ**:
- ä¼˜åŒ–å‰(NVIDIA+OpenAI):Â¥36,000/æœˆ
- ä¼˜åŒ–åŽ(æ˜‡è…¾+ç›˜å¤+å½’æ¡£):Â¥25,000/æœˆ
- **æ€»èŠ‚çœ**:Â¥11,000/æœˆ,å¹´èŠ‚çœ**Â¥132,000**

---

### 5.3 é«˜å¯ç”¨å’Œå®¹ç¾

**RTO/RPOç›®æ ‡**:
- RTO:< 30åˆ†é’Ÿ(GPUå®žä¾‹å¿«é€Ÿå¯åŠ¨)
- RPO:< 5åˆ†é’Ÿ(OBSå®žæ—¶å†™å…¥)

**é«˜å¯ç”¨æž¶æž„**:
```yaml
è®¡ç®—é«˜å¯ç”¨:
  - CCEèŠ‚ç‚¹åˆ†å¸ƒåœ¨3ä¸ªå¯ç”¨åŒº
  - GPUå®žä¾‹è·¨å¯ç”¨åŒºéƒ¨ç½²
  - è‡ªåŠ¨æ•…éšœè½¬ç§»

å­˜å‚¨é«˜å¯ç”¨:
  - OBSè·¨å¯ç”¨åŒºå¤šå‰¯æœ¬
  - å¼‚åœ°å¤‡ä»½(åŒ—äº¬å››â†’ä¸Šæµ·ä¸€)

APIé«˜å¯ç”¨:
  - ç›˜å¤APIå¤šåŒºåŸŸè´Ÿè½½å‡è¡¡
  - OpenAI APIå¤‡ç”¨
```

**å¤‡ä»½ç­–ç•¥**:
```yaml
æ•°æ®é›†: æ¯å‘¨å…¨é‡å¤‡ä»½åˆ°OBS
æ¨¡åž‹æƒé‡: ç‰ˆæœ¬æŽ§åˆ¶,ä¿ç•™æœ€è¿‘3ä¸ªç‰ˆæœ¬
å®žéªŒç»“æžœ: å®žæ—¶å†™å…¥OBS,ä¿ç•™90å¤©
æ¢å¤æ¼”ç»ƒ: æ¯æœˆ1æ¬¡
```

---

## 6. æ€»ç»“ä¸Žå†³ç­–å»ºè®®

### é€‚é…æ€§æ€»ç»“

| è¯„ä¼°ç»´åº¦ | è¯„åˆ† | è¯´æ˜Ž |
|---------|------|------|
| **æœåŠ¡è¦†ç›–åº¦** | â­â­â­â˜†â˜† 3/5 | åŸºç¡€æœåŠ¡å®Œå¤‡,GPUæŽ¨ç†éœ€é€‚é… |
| **æˆæœ¬ä¼˜åŠ¿** | â­â­â­â­â˜† 4/5 | æ¯”AWSä¾¿å®œ33-58%,ç›˜å¤æ¨¡åž‹èŠ‚çœ50%+ |
| **éƒ¨ç½²éš¾åº¦** | â­â­â˜†â˜†â˜† 2/5 | vLLM/FlashAttentioné€‚é…éœ€5-16äººå¤© |
| **è¿ç»´æˆæœ¬** | â­â­â­â˜†â˜† 3/5 | NVIDIA GPUå®žä¾‹ç®€å•,æ˜‡è…¾NPUéœ€ä¸“ä¸šè¿ç»´ |
| **æ€§èƒ½ä¿éšœ** | â­â­â­â­â˜† 4/5 | NVIDIA GPUæ€§èƒ½æœ€ä¼˜,æ˜‡è…¾ç•¥ä½Ž10-20% |
| **æ•°æ®åˆè§„** | â­â­â­â­â­ 5/5 | æ•°æ®ä¸å‡ºå¢ƒ,ç¬¦åˆç›‘ç®¡è¦æ±‚ |

**ç»¼åˆè¯„åˆ†**:â­â­â­â˜†â˜† **3.5/5** - **æŽ¨èéƒ¨ç½²(ä¼˜å…ˆNVIDIA GPU)**

---

### å†³ç­–å»ºè®®

#### âœ… å¼ºçƒˆæŽ¨èåŽä¸ºäº‘çš„åœºæ™¯

1. **æ•°æ®ä¸»æƒè¦æ±‚**:é‡‘èžã€æ”¿åŠ¡ã€åŒ»ç–—ç­‰è¡Œä¸š
2. **æˆæœ¬æ•æ„Ÿ**:é¢„ç®—æœ‰é™,éœ€é™ä½Ž30-60%äº‘æˆæœ¬
3. **ä¸­å›½å¸‚åœº**:ä¸»è¦æœåŠ¡ä¸­å›½ç”¨æˆ·
4. **å·²æœ‰åŽä¸ºäº‘åŸºç¡€è®¾æ–½**:å¯å¤ç”¨çŽ°æœ‰èµ„æº

#### âš ï¸ è°¨æ…Žè¯„ä¼°çš„åœºæ™¯

1. **æžè‡´æ€§èƒ½è¦æ±‚**:éœ€è¦æœ€ä¼˜GPUæŽ¨ç†æ€§èƒ½(å»ºè®®ç”¨NVIDIA GPUå®žä¾‹)
2. **å¿«é€ŸåŽŸåž‹éªŒè¯**:æ—¶é—´ç´§è¿«,æ— æ³•æŠ•å…¥é€‚é…å·¥ä½œé‡
3. **å…¨çƒéƒ¨ç½²**:éœ€è¦å…¨çƒå¤šåœ°åŸŸä½Žå»¶è¿Ÿ

---

### æœ€ç»ˆæŽ¨èæ–¹æ¡ˆ

**å°è§„æ¨¡(ç ”ç©¶å®žéªŒ)**:â­ æœ€æŽ¨è
```
éƒ¨ç½²: NVIDIA GPU ECS + BM25 CPU + ç›˜å¤API
æˆæœ¬: Â¥5,000-8,000/æœˆ
ä¼˜åŠ¿: é›¶é€‚é…,å¿«é€Ÿä¸Šçº¿,æ€§èƒ½æœ€ä¼˜
```

**ä¸­è§„æ¨¡(æŒç»­è¯„ä¼°)**:
```
éƒ¨ç½²: CCE + ModelArts + æ˜‡è…¾910B + ç›˜å¤API
æˆæœ¬: Â¥25,000-36,000/æœˆ
ä¼˜åŠ¿: æ‰˜ç®¡æœåŠ¡,æˆæœ¬æœ€ä¼˜,è‡ªåŠ¨æ‰©ç¼©å®¹
```

**å¤§è§„æ¨¡(ç”Ÿäº§çŽ¯å¢ƒ)**:
```
éƒ¨ç½²: CCEä¼ä¸šç‰ˆ + NVIDIA GPUé›†ç¾¤ + MindIE + RDS
æˆæœ¬: Â¥81,000-96,000/æœˆ
ä¼˜åŠ¿: ä¼ä¸šçº§å¯é æ€§,æ··åˆGPUæ–¹æ¡ˆ,é«˜æ€§èƒ½
```

---

### è¡ŒåŠ¨è®¡åˆ’

**ç«‹å³å¼€å§‹**(Day 1-7):
1. åˆ›å»ºåŽä¸ºäº‘è´¦å·,ç”³è¯·GPUé…é¢
2. åˆ›å»ºVPCç½‘ç»œ,é…ç½®å®‰å…¨ç»„
3. åˆ›å»ºOBS Bucket,ä¸Šä¼ æ•°æ®é›†
4. é‡‡è´­NVIDIA GPU ECSå®žä¾‹(æŽ¨èpi2.32xlarge.8)

**2å‘¨å†…å®Œæˆ**(Day 8-14):
1. å®‰è£…CUDAçŽ¯å¢ƒå’ŒPyTorch
2. éƒ¨ç½²vLLMæœåŠ¡
3. é…ç½®BM25æ£€ç´¢å’ŒDenseæ£€ç´¢
4. è¿è¡Œé¦–æ¬¡LongMemEval_Sè¯„ä¼°

**4å‘¨è¾¾åˆ°ç”Ÿäº§å°±ç»ª**(Day 15-28):
1. é…ç½®ç›‘æŽ§å‘Šè­¦(CES+APM)
2. å®žçŽ°ç›˜å¤APIæ›¿ä»£(å¯é€‰)
3. æ€§èƒ½è°ƒä¼˜å’Œæˆæœ¬ä¼˜åŒ–
4. å¤‡ä»½å’Œå®¹ç¾é…ç½®

**é¢„è®¡æ€»ä¸Šçº¿æ—¶é—´**:2-4å‘¨(NVIDIA GPU),4-8å‘¨(æ˜‡è…¾NPUé€‚é…)
**åˆå§‹æŠ•å…¥å·¥ä½œé‡**:5-10äººå¤©(NVIDIA GPU),14-26äººå¤©(æ˜‡è…¾NPU)

---

**é—®é¢˜å’¨è¯¢**:
- åŽä¸ºäº‘æŠ€æœ¯æ”¯æŒ:4000-955-988
- GPUå®žä¾‹ç”³è¯·:æäº¤å·¥å•ç”³è¯·é…é¢
- ç›˜å¤å¤§æ¨¡åž‹æŽ¥å…¥:æäº¤å·¥å•ç”³è¯·APIè®¿é—®æƒé™
- æ˜‡è…¾NPUé€‚é…:è”ç³»åŽä¸ºæ˜‡è…¾æŠ€æœ¯æ”¯æŒå›¢é˜Ÿ

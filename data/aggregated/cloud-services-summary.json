{
  "metadata": {
    "generated_at": "2026-02-13T16:10:25.155Z",
    "total_projects_analyzed": 25,
    "analysis_version": "2.0"
  },
  "cloud_service_usage_statistics": {
    "vector_database": {
      "total_projects": 43,
      "usage_percentage": 172
    },
    "graph_database": {
      "total_projects": 6,
      "usage_percentage": 24,
      "projects": [
        "MemOS",
        "Memary",
        "MemoryAgentBench",
        "cognee",
        "easymemory",
        "graphiti"
      ]
    },
    "object_storage": {
      "required": 0,
      "recommended": 3,
      "optional": 2,
      "total_percentage": 12,
      "projects": {
        "required": [],
        "recommended": [
          "MemOS",
          "cognee",
          "mem0"
        ],
        "optional": [
          "Memori",
          "ReMe"
        ]
      }
    },
    "gpu_acceleration": {
      "required": 1,
      "recommended": 8,
      "not_needed": 16,
      "projects": {
        "required": [
          "LongMemEval"
        ],
        "recommended": [
          "LightMem",
          "MemoryAgentBench",
          "SimpleMem",
          "easymemory",
          "general-agentic-memory",
          "langgraph-redis",
          "locomo",
          "memU"
        ],
        "notNeeded": [
          "A-MEM",
          "Backboard-Locomo-Benchmark",
          "MemOS",
          "Memary",
          "Memori",
          "ReMe",
          "beads",
          "claude-mem",
          "cognee",
          "graphiti",
          "hindsight",
          "letta",
          "mem0",
          "memory-agent",
          "memtrace",
          "supermemory"
        ]
      }
    }
  },
  "popular_tech_choices": {
    "vector_db_ranking": [
      {
        "name": "Qdrant",
        "count": 7,
        "rank": 1
      },
      {
        "name": "ChromaDB",
        "count": 5,
        "rank": 2
      },
      {
        "name": "LanceDB",
        "count": 3,
        "rank": 3
      },
      {
        "name": "Weaviate",
        "count": 2,
        "rank": 4
      },
      {
        "name": "pgvector",
        "count": 2,
        "rank": 5
      },
      {
        "name": "Backboard Cloud",
        "count": 1,
        "rank": 6
      },
      {
        "name": "内存",
        "count": 1,
        "rank": 7
      },
      {
        "name": "无（可选集成Pinecone",
        "count": 1,
        "rank": 8
      },
      {
        "name": "Milvus）",
        "count": 1,
        "rank": 9
      },
      {
        "name": "faiss-cpu",
        "count": 1,
        "rank": 10
      }
    ]
  },
  "deployment_patterns": {
    "containerization": {
      "docker": {
        "total_projects": 22,
        "percentage": 88
      }
    }
  },
  "cost_analysis": {
    "cost_breakdown_by_category": {
      "avg_percentages": {
        "llm_api": "60-80%",
        "compute": "10-20%",
        "storage": "5-10%",
        "network": "5-10%"
      }
    },
    "huawei_cloud_cost_estimates": {
      "small_scale": {
        "min": 1000,
        "max": 5000,
        "projects_analyzed": 25
      },
      "medium_scale": {
        "min": 5000,
        "max": 25000,
        "projects_analyzed": 25
      }
    }
  },
  "key_insights": [
    "6 projects require graph databases (Neo4j/FalkorDB/Kuzu)",
    "0 projects require object storage (S3/OBS) for core functionality",
    "1 projects require GPU, 8 recommend GPU acceleration",
    "22 projects support Docker containerization (88%)",
    "Vector databases adoption: 172% of projects"
  ],
  "recommended_stacks": {
    "huawei_cloud": {
      "description": "Huawei Cloud deployment stack for Agent Memory projects",
      "services": {
        "compute": "ECS (8-32 vCPU) or FunctionGraph for serverless",
        "vector_database": "PostgreSQL + pgvector or self-hosted Qdrant/Milvus",
        "graph_database": "Self-hosted Neo4j on ECS (no managed service available)",
        "object_storage": "OBS (S3-compatible API)",
        "cache": "DCS Redis",
        "llm_api": "Pangu Models or OpenAI via NAT Gateway"
      },
      "challenges": [
        "No managed Neo4j service - requires self-hosting",
        "GPU instances or Ascend NPU for GPU workloads",
        "NPU migration requires 1-2 weeks for CUDA-dependent projects"
      ]
    }
  },
  "optimization_strategies": [
    "Use object storage tiering (hot/warm/cold) to reduce costs by 70%",
    "Consider local embedding models to reduce API costs",
    "Batch processing for embedding generation",
    "Use smaller LLM models (e.g., GPT-4o-mini) for non-critical operations",
    "Implement caching for frequently accessed memories"
  ]
}